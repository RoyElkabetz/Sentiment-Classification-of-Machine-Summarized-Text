{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMj5p66KlY+F7ygB25IEhQ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoyElkabetz/Text-Summarization-with-Deep-Learning/blob/main/Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KMfu5tAbJrz",
        "outputId": "c600c4e3-a5e9-4f66-89df-17ed32a9d815"
      },
      "source": [
        "## uncomment only if running from google.colab\n",
        "# clone the git reposetory\n",
        "!git clone https://github.com/RoyElkabetz/Text-Summarization-with-Deep-Learning\n",
        "# add path to .py files for import\n",
        "import sys\n",
        "sys.path.insert(1, \"/content/Text-Summarization-with-Deep-Learning\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Text-Summarization-with-Deep-Learning'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 53 (delta 28), reused 6 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (53/53), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBIVxaAlSFD3",
        "outputId": "d98915f9-90f6-4eb6-9646-ee6dd38f0ff3"
      },
      "source": [
        "## uncomment if you want to mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F96lfuaZWBI6",
        "outputId": "0d79fa5b-9d7a-4973-f0f7-2ad005761c5b"
      },
      "source": [
        "%matplotlib inline\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torchtext.datasets import IMDB\n",
        "import torchtext.data as data\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f'torch {torch.__version__}')\n",
        "print('Device properties:')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    gpu_data = torch.cuda.get_device_properties(0)\n",
        "    gpu_name = gpu_data.name\n",
        "    gpu_mem  = f'{gpu_data.total_memory * 1e-9:.02f} Gb'\n",
        "    print(f'GPU: {gpu_name}\\nMemory: {gpu_mem}')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('CPU')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch 1.9.0+cu102\n",
            "Device properties:\n",
            "GPU: Tesla T4\n",
            "Memory: 15.84 Gb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj1U-8ysnXbA"
      },
      "source": [
        "class DataFrameDataset(Dataset):\n",
        "  \"\"\"Create a torch.utils.data.Dataset from a pandas.DataFrame or a CSV file.\"\"\"\n",
        "\n",
        "  def __init__(self, csv_file_path=None, pd_dataframe=None, only_columns=None):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "      csv_file_path (string): Path to the csv file with annotations.\n",
        "      pd_dataframe (Pandas DataFrame): A Pandas DataFrame with containing the\n",
        "      data.\n",
        "      only_columns (list): A List of colums names from the data. \n",
        "    \"\"\"\n",
        "    if isinstance(pd_dataframe, pd.DataFrame):\n",
        "      self.df = pd_dataframe \n",
        "    else:\n",
        "      self.df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    if only_columns is not None:\n",
        "      if isinstance(only_columns, list):\n",
        "        for item in only_columns:\n",
        "          if item not in self.df.columns:\n",
        "            raise ValueError(f\"Got a column name '{item}' in only_columns which is not in DataFrame columns.\")\n",
        "        self.only_columns = only_columns\n",
        "      else:\n",
        "        raise TypeError(f\"only_columns must be a <class 'list'>, instead got a {type(only_columns)}.\")\n",
        "    else:\n",
        "      self.only_columns = list(self.df.columns)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    row = self.df.iloc[idx][self.only_columns]\n",
        "    row_list = [item for item in row]\n",
        "    return row_list"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD5sdelTP2ex"
      },
      "source": [
        "## Get the IMDB dataset and create a vocabulary from the train dataset\n",
        "I use the IMDB test data as train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fQONvlkWN7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc2c55e6-84fe-47d1-ee2d-42d254428166"
      },
      "source": [
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = IMDB(split='test')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\", \"<sos>\", \"<eos>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 60.8MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjck56H3QpuU"
      },
      "source": [
        "## Create text and labels pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ModijEJY714"
      },
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: 0 if x=='neg' else 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ8CR_ktQx1G"
      },
      "source": [
        "## Print some random samples and the size of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AoszQTWWN-a",
        "outputId": "0ea417fe-8dd7-4aaa-a22e-1bd8d1fbe7e6"
      },
      "source": [
        "train_iter = IMDB(split='test')\n",
        "n_samples = len(train_iter)\n",
        "random_list = torch.randint(0, n_samples - 1, (4, ))\n",
        "labels = []\n",
        "for i, (label, text) in enumerate(train_iter):\n",
        "    labels.append(label)\n",
        "    if i in random_list:\n",
        "        print(f'Label: {label_pipeline(label)}')\n",
        "        print(f'Text: {text}')\n",
        "        print(f'Split: {tokenizer(text)}')\n",
        "        print(f'Tokens: {text_pipeline(text)}\\n')\n",
        "print('Number of classes: {}'.format(len(set(labels))))\n",
        "print('Number of samples: {}'.format(n_samples))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 0\n",
            "Text: I must be honest, I like romantic comedies, but this was not what I had hoped for. I thought Ellen Degeneres was having the biggest part, which should have been, because I didn't like the two struggling bed partners. It was awful. Poor Tom Selleck!! He had to act with someone who was that much in the picture while it should have been him and Ellen to be in most of the film. They were the only believable ones. And the only really funny parts starred them, not Kate Capshaw and that Everett guy.. Cool that mummy is coming out of the closet, I thought that was a nice surprise. <br /><br />I'm just glad I saw it on the cable and I didn't pay any money renting it..\n",
            "Split: ['i', 'must', 'be', 'honest', ',', 'i', 'like', 'romantic', 'comedies', ',', 'but', 'this', 'was', 'not', 'what', 'i', 'had', 'hoped', 'for', '.', 'i', 'thought', 'ellen', 'degeneres', 'was', 'having', 'the', 'biggest', 'part', ',', 'which', 'should', 'have', 'been', ',', 'because', 'i', 'didn', \"'\", 't', 'like', 'the', 'two', 'struggling', 'bed', 'partners', '.', 'it', 'was', 'awful', '.', 'poor', 'tom', 'selleck', '!', '!', 'he', 'had', 'to', 'act', 'with', 'someone', 'who', 'was', 'that', 'much', 'in', 'the', 'picture', 'while', 'it', 'should', 'have', 'been', 'him', 'and', 'ellen', 'to', 'be', 'in', 'most', 'of', 'the', 'film', '.', 'they', 'were', 'the', 'only', 'believable', 'ones', '.', 'and', 'the', 'only', 'really', 'funny', 'parts', 'starred', 'them', ',', 'not', 'kate', 'capshaw', 'and', 'that', 'everett', 'guy', '.', '.', 'cool', 'that', 'mummy', 'is', 'coming', 'out', 'of', 'the', 'closet', ',', 'i', 'thought', 'that', 'was', 'a', 'nice', 'surprise', '.', 'i', \"'\", 'm', 'just', 'glad', 'i', 'saw', 'it', 'on', 'the', 'cable', 'and', 'i', 'didn', \"'\", 't', 'pay', 'any', 'money', 'renting', 'it', '.', '.']\n",
            "Tokens: [14, 217, 35, 1145, 5, 14, 46, 768, 1341, 5, 23, 15, 18, 30, 56, 14, 76, 3780, 22, 4, 14, 200, 5339, 13611, 18, 274, 3, 1112, 181, 5, 73, 151, 34, 86, 5, 97, 14, 160, 10, 29, 46, 3, 121, 3173, 1083, 6789, 4, 12, 18, 379, 4, 336, 851, 5765, 37, 37, 32, 76, 9, 501, 21, 292, 43, 18, 16, 81, 13, 3, 446, 146, 12, 151, 34, 86, 96, 6, 5339, 9, 35, 13, 98, 8, 3, 24, 4, 40, 78, 3, 74, 790, 675, 4, 6, 3, 74, 72, 161, 503, 2943, 100, 5, 30, 3412, 9057, 6, 16, 5556, 208, 4, 4, 554, 16, 3014, 11, 553, 52, 8, 3, 4344, 5, 14, 200, 16, 18, 7, 342, 815, 4, 14, 10, 150, 49, 1218, 14, 216, 12, 28, 3, 2016, 6, 14, 160, 10, 29, 963, 109, 308, 2486, 12, 4, 4]\n",
            "\n",
            "Label: 1\n",
            "Text: I have to admit that when I first heard about the Apocalypse film it was a worry.<br /><br />I mean, they have a lot to live up to, don't they? When they first did a stage show they won the Perrier award and when they did radio they won a Sony award. When they ventured onto our telly's they won a Bafta award, a Royal Television Society Award and the Golden Rose of Montreux.<br /><br />When the first series aired in January 1999 it was mind-blowing! A real breath of fresh air in British Comedy, and when the second series aired a year later it built on that foundation and sealed the shows cult status around the world, our web stats show that we have received visitors from every single country on the planet! The 'Local show for Local People' showcased the Gents talent for live performance and opened doors for the gents to do more live performing such as 'Art' in the west end.<br /><br />The fans favourite has always been the Christmas Special, less of a sketch show and more a tribute to classic horror films yet still wrapped up in the delicious League style.<br /><br />And then of course there was the 'difficult' third series, still a hit with the loyal hardcore fans of course, but maybe a little bit ahead of its time for a mainstream TV audience.<br /><br />As I say, a lot to live up to.<br /><br />So now we have the film and...Well a film is different isn't it? It will be seen by much larger numbers than the radio or TV shows and with the third series in mind I was worried.<br /><br />Well as you know I was lucky enough to get to see the film yesterday at a press screening in London and all my doubts were blown away (literally) in the first few minutes! I am not going to give plot lines away as some reviewers have done, nor am I going to tell you the catch phrases (although there is really only one) but I will try to tell you what they have managed to achieve with this film! Leaving the cinema on Monday night I could only imagine writing 'Oh my god, it's brilliant, its amazing, its the best thing they have ever done, better than the first, second and specials all rolled into one!' Of course I owe my visitors a much better explanation than that! So, why is it brilliant? This is a film for everyone, the casual fan, the obsessive fan the occasional fan and even for someone who is sat in the wrong cinema! You don't have to have watched the series to enjoy this film; it works on so many levels.<br /><br />This film reminded me why I am a League of Gentlemen Fan! You can tell that filming was a true labour of love too; the attention to detail is incredible. The sets for the TV show were always detailed but I am going to have to watch the film again just to look at the background! The story moves at a swift pace, the action carrying us from Royston Vasey to the real world where we meet the 'Creators' who are of course the League themselves! Along the way we manage to bump into favourite characters from the show but always within the central story unlike the TV sketch show.<br /><br />I was glad that the film was dark in places, a little scary and a little strange...only fitting for The League of Gentlemen. The Gents also managed to get their revenge on the BBC censors, not as much slipping in the word 'Mongoloid' as screaming it from the roof tops! Some may think the Gents portrayal of themselves a little indulgent but that's the joke and with that comes my only worry, the in jokes I mention below may puzzle some viewers and they might come over a little too clever...but I shouldn't worry, there is always a poo joke waiting just around the corner and speaking of jokes, they come thick and fast, and in a mixture of clever references, wig jokes, bum jokes, visual jokes and cock gags! I haven't laughed out loud in a cinema since...well, I can't remember! The fans that have been 'with' the League since the beginning are rewarded with loads of 'in' jokes, some that work on two levels, a mainstream audience may laugh at a reference to a compact disc for one reason whilst fans of the Local show will laugh for another reason altogether! The cameos are genius! Peter Kay and Simon Pegg form the strangest double act you have ever seen, Simon getting one of the films biggest laughs just by making a noise! I was a little worried about the 1690's aspect of the film when I first heard about it but as a story within a story I was just getting into it when...but that would be telling! All I need to say is that it fits wonderfully and adds to the overall feel of the film! I am not a professional reviewer of films, so I am finding it difficult to put into words how much I enjoyed this film but for now I will just say that if the supposed benchmark for British Comedy films in recent years was the excellent 'Shaun of the Dead' then I am sorry but a new benchmark has just been set by the inventive, hilarious and sometimes a little scary...The League of Gentlemen's Apocalypse.<br /><br />Jason Kenny 2005\n",
            "Split: ['i', 'have', 'to', 'admit', 'that', 'when', 'i', 'first', 'heard', 'about', 'the', 'apocalypse', 'film', 'it', 'was', 'a', 'worry', '.', 'i', 'mean', ',', 'they', 'have', 'a', 'lot', 'to', 'live', 'up', 'to', ',', 'don', \"'\", 't', 'they', '?', 'when', 'they', 'first', 'did', 'a', 'stage', 'show', 'they', 'won', 'the', 'perrier', 'award', 'and', 'when', 'they', 'did', 'radio', 'they', 'won', 'a', 'sony', 'award', '.', 'when', 'they', 'ventured', 'onto', 'our', 'telly', \"'\", 's', 'they', 'won', 'a', 'bafta', 'award', ',', 'a', 'royal', 'television', 'society', 'award', 'and', 'the', 'golden', 'rose', 'of', 'montreux', '.', 'when', 'the', 'first', 'series', 'aired', 'in', 'january', '1999', 'it', 'was', 'mind-blowing', '!', 'a', 'real', 'breath', 'of', 'fresh', 'air', 'in', 'british', 'comedy', ',', 'and', 'when', 'the', 'second', 'series', 'aired', 'a', 'year', 'later', 'it', 'built', 'on', 'that', 'foundation', 'and', 'sealed', 'the', 'shows', 'cult', 'status', 'around', 'the', 'world', ',', 'our', 'web', 'stats', 'show', 'that', 'we', 'have', 'received', 'visitors', 'from', 'every', 'single', 'country', 'on', 'the', 'planet', '!', 'the', \"'\", 'local', 'show', 'for', 'local', 'people', \"'\", 'showcased', 'the', 'gents', 'talent', 'for', 'live', 'performance', 'and', 'opened', 'doors', 'for', 'the', 'gents', 'to', 'do', 'more', 'live', 'performing', 'such', 'as', \"'\", 'art', \"'\", 'in', 'the', 'west', 'end', '.', 'the', 'fans', 'favourite', 'has', 'always', 'been', 'the', 'christmas', 'special', ',', 'less', 'of', 'a', 'sketch', 'show', 'and', 'more', 'a', 'tribute', 'to', 'classic', 'horror', 'films', 'yet', 'still', 'wrapped', 'up', 'in', 'the', 'delicious', 'league', 'style', '.', 'and', 'then', 'of', 'course', 'there', 'was', 'the', \"'\", 'difficult', \"'\", 'third', 'series', ',', 'still', 'a', 'hit', 'with', 'the', 'loyal', 'hardcore', 'fans', 'of', 'course', ',', 'but', 'maybe', 'a', 'little', 'bit', 'ahead', 'of', 'its', 'time', 'for', 'a', 'mainstream', 'tv', 'audience', '.', 'as', 'i', 'say', ',', 'a', 'lot', 'to', 'live', 'up', 'to', '.', 'so', 'now', 'we', 'have', 'the', 'film', 'and', '.', '.', '.', 'well', 'a', 'film', 'is', 'different', 'isn', \"'\", 't', 'it', '?', 'it', 'will', 'be', 'seen', 'by', 'much', 'larger', 'numbers', 'than', 'the', 'radio', 'or', 'tv', 'shows', 'and', 'with', 'the', 'third', 'series', 'in', 'mind', 'i', 'was', 'worried', '.', 'well', 'as', 'you', 'know', 'i', 'was', 'lucky', 'enough', 'to', 'get', 'to', 'see', 'the', 'film', 'yesterday', 'at', 'a', 'press', 'screening', 'in', 'london', 'and', 'all', 'my', 'doubts', 'were', 'blown', 'away', '(', 'literally', ')', 'in', 'the', 'first', 'few', 'minutes', '!', 'i', 'am', 'not', 'going', 'to', 'give', 'plot', 'lines', 'away', 'as', 'some', 'reviewers', 'have', 'done', ',', 'nor', 'am', 'i', 'going', 'to', 'tell', 'you', 'the', 'catch', 'phrases', '(', 'although', 'there', 'is', 'really', 'only', 'one', ')', 'but', 'i', 'will', 'try', 'to', 'tell', 'you', 'what', 'they', 'have', 'managed', 'to', 'achieve', 'with', 'this', 'film', '!', 'leaving', 'the', 'cinema', 'on', 'monday', 'night', 'i', 'could', 'only', 'imagine', 'writing', \"'\", 'oh', 'my', 'god', ',', 'it', \"'\", 's', 'brilliant', ',', 'its', 'amazing', ',', 'its', 'the', 'best', 'thing', 'they', 'have', 'ever', 'done', ',', 'better', 'than', 'the', 'first', ',', 'second', 'and', 'specials', 'all', 'rolled', 'into', 'one', '!', \"'\", 'of', 'course', 'i', 'owe', 'my', 'visitors', 'a', 'much', 'better', 'explanation', 'than', 'that', '!', 'so', ',', 'why', 'is', 'it', 'brilliant', '?', 'this', 'is', 'a', 'film', 'for', 'everyone', ',', 'the', 'casual', 'fan', ',', 'the', 'obsessive', 'fan', 'the', 'occasional', 'fan', 'and', 'even', 'for', 'someone', 'who', 'is', 'sat', 'in', 'the', 'wrong', 'cinema', '!', 'you', 'don', \"'\", 't', 'have', 'to', 'have', 'watched', 'the', 'series', 'to', 'enjoy', 'this', 'film', 'it', 'works', 'on', 'so', 'many', 'levels', '.', 'this', 'film', 'reminded', 'me', 'why', 'i', 'am', 'a', 'league', 'of', 'gentlemen', 'fan', '!', 'you', 'can', 'tell', 'that', 'filming', 'was', 'a', 'true', 'labour', 'of', 'love', 'too', 'the', 'attention', 'to', 'detail', 'is', 'incredible', '.', 'the', 'sets', 'for', 'the', 'tv', 'show', 'were', 'always', 'detailed', 'but', 'i', 'am', 'going', 'to', 'have', 'to', 'watch', 'the', 'film', 'again', 'just', 'to', 'look', 'at', 'the', 'background', '!', 'the', 'story', 'moves', 'at', 'a', 'swift', 'pace', ',', 'the', 'action', 'carrying', 'us', 'from', 'royston', 'vasey', 'to', 'the', 'real', 'world', 'where', 'we', 'meet', 'the', \"'\", 'creators', \"'\", 'who', 'are', 'of', 'course', 'the', 'league', 'themselves', '!', 'along', 'the', 'way', 'we', 'manage', 'to', 'bump', 'into', 'favourite', 'characters', 'from', 'the', 'show', 'but', 'always', 'within', 'the', 'central', 'story', 'unlike', 'the', 'tv', 'sketch', 'show', '.', 'i', 'was', 'glad', 'that', 'the', 'film', 'was', 'dark', 'in', 'places', ',', 'a', 'little', 'scary', 'and', 'a', 'little', 'strange', '.', '.', '.', 'only', 'fitting', 'for', 'the', 'league', 'of', 'gentlemen', '.', 'the', 'gents', 'also', 'managed', 'to', 'get', 'their', 'revenge', 'on', 'the', 'bbc', 'censors', ',', 'not', 'as', 'much', 'slipping', 'in', 'the', 'word', \"'\", 'mongoloid', \"'\", 'as', 'screaming', 'it', 'from', 'the', 'roof', 'tops', '!', 'some', 'may', 'think', 'the', 'gents', 'portrayal', 'of', 'themselves', 'a', 'little', 'indulgent', 'but', 'that', \"'\", 's', 'the', 'joke', 'and', 'with', 'that', 'comes', 'my', 'only', 'worry', ',', 'the', 'in', 'jokes', 'i', 'mention', 'below', 'may', 'puzzle', 'some', 'viewers', 'and', 'they', 'might', 'come', 'over', 'a', 'little', 'too', 'clever', '.', '.', '.', 'but', 'i', 'shouldn', \"'\", 't', 'worry', ',', 'there', 'is', 'always', 'a', 'poo', 'joke', 'waiting', 'just', 'around', 'the', 'corner', 'and', 'speaking', 'of', 'jokes', ',', 'they', 'come', 'thick', 'and', 'fast', ',', 'and', 'in', 'a', 'mixture', 'of', 'clever', 'references', ',', 'wig', 'jokes', ',', 'bum', 'jokes', ',', 'visual', 'jokes', 'and', 'cock', 'gags', '!', 'i', 'haven', \"'\", 't', 'laughed', 'out', 'loud', 'in', 'a', 'cinema', 'since', '.', '.', '.', 'well', ',', 'i', 'can', \"'\", 't', 'remember', '!', 'the', 'fans', 'that', 'have', 'been', \"'\", 'with', \"'\", 'the', 'league', 'since', 'the', 'beginning', 'are', 'rewarded', 'with', 'loads', 'of', \"'\", 'in', \"'\", 'jokes', ',', 'some', 'that', 'work', 'on', 'two', 'levels', ',', 'a', 'mainstream', 'audience', 'may', 'laugh', 'at', 'a', 'reference', 'to', 'a', 'compact', 'disc', 'for', 'one', 'reason', 'whilst', 'fans', 'of', 'the', 'local', 'show', 'will', 'laugh', 'for', 'another', 'reason', 'altogether', '!', 'the', 'cameos', 'are', 'genius', '!', 'peter', 'kay', 'and', 'simon', 'pegg', 'form', 'the', 'strangest', 'double', 'act', 'you', 'have', 'ever', 'seen', ',', 'simon', 'getting', 'one', 'of', 'the', 'films', 'biggest', 'laughs', 'just', 'by', 'making', 'a', 'noise', '!', 'i', 'was', 'a', 'little', 'worried', 'about', 'the', '1690', \"'\", 's', 'aspect', 'of', 'the', 'film', 'when', 'i', 'first', 'heard', 'about', 'it', 'but', 'as', 'a', 'story', 'within', 'a', 'story', 'i', 'was', 'just', 'getting', 'into', 'it', 'when', '.', '.', '.', 'but', 'that', 'would', 'be', 'telling', '!', 'all', 'i', 'need', 'to', 'say', 'is', 'that', 'it', 'fits', 'wonderfully', 'and', 'adds', 'to', 'the', 'overall', 'feel', 'of', 'the', 'film', '!', 'i', 'am', 'not', 'a', 'professional', 'reviewer', 'of', 'films', ',', 'so', 'i', 'am', 'finding', 'it', 'difficult', 'to', 'put', 'into', 'words', 'how', 'much', 'i', 'enjoyed', 'this', 'film', 'but', 'for', 'now', 'i', 'will', 'just', 'say', 'that', 'if', 'the', 'supposed', 'benchmark', 'for', 'british', 'comedy', 'films', 'in', 'recent', 'years', 'was', 'the', 'excellent', \"'\", 'shaun', 'of', 'the', 'dead', \"'\", 'then', 'i', 'am', 'sorry', 'but', 'a', 'new', 'benchmark', 'has', 'just', 'been', 'set', 'by', 'the', 'inventive', ',', 'hilarious', 'and', 'sometimes', 'a', 'little', 'scary', '.', '.', '.', 'the', 'league', 'of', 'gentlemen', \"'\", 's', 'apocalypse', '.', 'jason', 'kenny', '2005']\n",
            "Tokens: [14, 34, 9, 985, 16, 60, 14, 99, 546, 51, 3, 3337, 24, 12, 18, 7, 3006, 4, 14, 397, 5, 40, 34, 7, 171, 9, 452, 66, 9, 5, 95, 10, 29, 40, 54, 60, 40, 99, 122, 7, 1033, 124, 40, 396, 3, 37575, 1434, 6, 60, 40, 122, 1241, 40, 396, 7, 11271, 1434, 4, 60, 40, 17938, 1522, 262, 7448, 10, 17, 40, 396, 7, 21338, 1434, 5, 7, 4888, 686, 843, 1434, 6, 3, 1703, 2240, 8, 78713, 4, 60, 3, 99, 229, 3067, 13, 8764, 3599, 12, 18, 12119, 37, 7, 159, 3068, 8, 1423, 902, 13, 724, 212, 5, 6, 60, 3, 354, 229, 3067, 7, 350, 315, 12, 2189, 28, 16, 6073, 6, 14395, 3, 273, 1326, 2579, 194, 3, 188, 5, 262, 3764, 32903, 124, 16, 79, 34, 2059, 9203, 44, 178, 711, 644, 28, 3, 1451, 37, 3, 10, 689, 124, 22, 689, 91, 10, 10205, 3, 14188, 606, 22, 452, 251, 6, 2957, 3476, 22, 3, 14188, 9, 89, 61, 452, 3528, 149, 19, 10, 550, 10, 13, 3, 1234, 138, 4, 3, 476, 1717, 55, 222, 86, 3, 1437, 313, 5, 383, 8, 7, 6884, 124, 6, 61, 7, 3036, 9, 365, 199, 114, 250, 143, 3997, 66, 13, 3, 6488, 2890, 443, 4, 6, 102, 8, 272, 47, 18, 3, 10, 862, 10, 940, 229, 5, 143, 7, 646, 21, 3, 4166, 3556, 476, 8, 272, 5, 23, 270, 7, 129, 236, 1543, 8, 101, 69, 22, 7, 2639, 249, 311, 4, 19, 14, 139, 5, 7, 171, 9, 452, 66, 9, 4, 45, 156, 79, 34, 3, 24, 6, 4, 4, 4, 80, 7, 24, 11, 289, 218, 10, 29, 12, 54, 12, 90, 35, 117, 41, 81, 3940, 1951, 82, 3, 1241, 48, 249, 273, 6, 21, 3, 940, 229, 13, 340, 14, 18, 3729, 4, 80, 19, 26, 123, 14, 18, 1950, 197, 9, 85, 9, 70, 3, 24, 4185, 38, 7, 3753, 2620, 13, 1300, 6, 39, 64, 5612, 78, 2623, 248, 27, 1188, 25, 13, 3, 99, 179, 234, 37, 14, 231, 30, 174, 9, 207, 127, 416, 248, 19, 57, 2191, 34, 228, 5, 954, 231, 14, 174, 9, 387, 26, 3, 1356, 8961, 27, 266, 47, 11, 72, 74, 36, 25, 23, 14, 90, 359, 9, 387, 26, 56, 40, 34, 1312, 9, 2914, 21, 15, 24, 37, 1187, 3, 448, 28, 10149, 326, 14, 103, 74, 776, 506, 10, 434, 64, 524, 5, 12, 10, 17, 502, 5, 101, 507, 5, 101, 3, 128, 155, 40, 34, 131, 228, 5, 134, 82, 3, 99, 5, 354, 6, 10988, 39, 4328, 93, 36, 37, 10, 8, 272, 14, 9345, 64, 9203, 7, 81, 134, 1709, 82, 16, 37, 45, 5, 145, 11, 12, 502, 54, 15, 11, 7, 24, 22, 295, 5, 3, 5044, 327, 5, 3, 6981, 327, 3, 2777, 327, 6, 67, 22, 292, 43, 11, 1713, 13, 3, 360, 448, 37, 26, 95, 10, 29, 34, 9, 34, 291, 3, 229, 9, 355, 15, 24, 12, 511, 28, 45, 116, 1881, 4, 15, 24, 1538, 77, 145, 14, 231, 7, 2890, 8, 5000, 327, 37, 26, 58, 387, 16, 1311, 18, 7, 307, 13747, 8, 120, 108, 3, 678, 9, 1487, 11, 1074, 4, 3, 738, 22, 3, 249, 124, 78, 222, 3319, 23, 14, 231, 174, 9, 34, 9, 113, 3, 24, 180, 49, 9, 170, 38, 3, 915, 37, 3, 75, 1131, 38, 7, 6713, 1021, 5, 3, 233, 3088, 190, 44, 14380, 12252, 9, 3, 159, 188, 126, 79, 877, 3, 10, 3407, 10, 43, 31, 8, 272, 3, 2890, 531, 37, 331, 3, 105, 79, 1833, 9, 8871, 93, 1717, 111, 44, 3, 124, 23, 222, 754, 3, 1415, 75, 1016, 3, 249, 6884, 124, 4, 14, 18, 1218, 16, 3, 24, 18, 463, 13, 1318, 5, 7, 129, 700, 6, 7, 129, 684, 4, 4, 4, 74, 4159, 22, 3, 2890, 8, 5000, 4, 3, 14188, 92, 1312, 9, 85, 71, 1307, 28, 3, 1898, 10792, 5, 30, 19, 81, 12610, 13, 3, 661, 10, 31861, 10, 19, 2185, 12, 44, 3, 5199, 5113, 37, 57, 211, 112, 3, 14188, 1075, 8, 531, 7, 129, 8925, 23, 16, 10, 17, 3, 966, 6, 21, 16, 290, 64, 74, 3006, 5, 3, 13, 604, 14, 725, 1763, 211, 4923, 57, 809, 6, 40, 237, 223, 144, 7, 129, 108, 960, 4, 4, 4, 23, 14, 1523, 10, 29, 3006, 5, 47, 11, 222, 7, 12549, 966, 1035, 49, 194, 3, 3548, 6, 1469, 8, 604, 5, 40, 223, 4281, 6, 875, 5, 6, 13, 7, 3854, 8, 960, 1651, 5, 5591, 604, 5, 8409, 604, 5, 1020, 604, 6, 16454, 1656, 37, 14, 712, 10, 29, 1363, 52, 1441, 13, 7, 448, 240, 4, 4, 4, 80, 5, 14, 58, 10, 29, 393, 37, 3, 476, 16, 34, 86, 10, 21, 10, 3, 2890, 240, 3, 456, 31, 6797, 21, 4103, 8, 10, 13, 10, 604, 5, 57, 16, 175, 28, 121, 1881, 5, 7, 2639, 311, 211, 409, 38, 7, 2864, 9, 7, 17277, 4006, 22, 36, 298, 1748, 476, 8, 3, 689, 124, 90, 409, 22, 164, 298, 3227, 37, 3, 3772, 31, 1134, 37, 786, 6768, 6, 3238, 16087, 825, 3, 8996, 1803, 501, 26, 34, 131, 117, 5, 3238, 377, 36, 8, 3, 114, 1112, 919, 49, 41, 269, 7, 3814, 37, 14, 18, 7, 129, 3729, 51, 3, 33629, 10, 17, 1324, 8, 3, 24, 60, 14, 99, 546, 51, 12, 23, 19, 7, 75, 754, 7, 75, 14, 18, 49, 377, 93, 12, 60, 4, 4, 4, 23, 16, 68, 35, 1044, 37, 39, 14, 361, 9, 139, 11, 16, 12, 2355, 1816, 6, 1426, 9, 3, 442, 241, 8, 3, 24, 37, 14, 231, 30, 7, 1674, 2221, 8, 114, 5, 45, 14, 231, 1365, 12, 862, 9, 279, 93, 639, 94, 81, 14, 516, 15, 24, 23, 22, 156, 14, 90, 49, 139, 16, 50, 3, 470, 16384, 22, 724, 212, 114, 13, 1189, 166, 18, 3, 320, 10, 11553, 8, 3, 367, 10, 102, 14, 231, 716, 23, 7, 184, 16384, 55, 49, 86, 301, 41, 3, 3847, 5, 537, 6, 547, 7, 129, 700, 4, 4, 4, 3, 2890, 8, 5000, 10, 17, 3337, 4, 1532, 12095, 2824]\n",
            "\n",
            "Label: 1\n",
            "Text: What happened to Ava Gardner in the 1940s and Marilyn Monroe in the '50s also seemed to take place for modern-day actress Michelle Pfeiffer in the '80s: Her remarkable good looks got in the way of her being taken seriously as an accomplished, superbly talented actress. Anyone looking for validation of Pfeiffer's dramatic abilities need look no further than her work in 1991's \"Frankie and Johnny\" or '92's \"Love Field\" (a personal favorite of mine); those looking to see what a splendid comedic actress she can be, when given the right part, should check out 1988's \"Married to the Mob.\" In this one, she plays Angela Demarco, the widow of a recently \"iced\" Mob hit-man, who moves from her garishly tacky Long Island home to start a new life for herself and her son, while being pursued by Mob boss Dean Stockwell and FBI man Matthew Modine. While this movie has lots going for it (a very amusing script; offbeat characters; sudden sharp turns to unexpected violence, as in director Jonathan Demme's previous effort \"Something Wild\"; and hilarious yet menacing performances by Stockwell and Mercedes Ruehl, as his jealous wife from hell), Michelle steals the show easily. Notice how perfectly she nails Angela's undereducated, Long Island Italian accent, and the many fine mannerisms that she brings to the role to really flesh out this spunky and surprisingly bright character. Once upon a time, long ago, Oscars were handed out to actresses for comedic roles such as this one. Had this film been made 60 years ago, Michelle mighta been a contenduh...\n",
            "Split: ['what', 'happened', 'to', 'ava', 'gardner', 'in', 'the', '1940s', 'and', 'marilyn', 'monroe', 'in', 'the', \"'\", '50s', 'also', 'seemed', 'to', 'take', 'place', 'for', 'modern-day', 'actress', 'michelle', 'pfeiffer', 'in', 'the', \"'\", '80s', 'her', 'remarkable', 'good', 'looks', 'got', 'in', 'the', 'way', 'of', 'her', 'being', 'taken', 'seriously', 'as', 'an', 'accomplished', ',', 'superbly', 'talented', 'actress', '.', 'anyone', 'looking', 'for', 'validation', 'of', 'pfeiffer', \"'\", 's', 'dramatic', 'abilities', 'need', 'look', 'no', 'further', 'than', 'her', 'work', 'in', '1991', \"'\", 's', 'frankie', 'and', 'johnny', 'or', \"'\", '92', \"'\", 's', 'love', 'field', '(', 'a', 'personal', 'favorite', 'of', 'mine', ')', 'those', 'looking', 'to', 'see', 'what', 'a', 'splendid', 'comedic', 'actress', 'she', 'can', 'be', ',', 'when', 'given', 'the', 'right', 'part', ',', 'should', 'check', 'out', '1988', \"'\", 's', 'married', 'to', 'the', 'mob', '.', 'in', 'this', 'one', ',', 'she', 'plays', 'angela', 'demarco', ',', 'the', 'widow', 'of', 'a', 'recently', 'iced', 'mob', 'hit-man', ',', 'who', 'moves', 'from', 'her', 'garishly', 'tacky', 'long', 'island', 'home', 'to', 'start', 'a', 'new', 'life', 'for', 'herself', 'and', 'her', 'son', ',', 'while', 'being', 'pursued', 'by', 'mob', 'boss', 'dean', 'stockwell', 'and', 'fbi', 'man', 'matthew', 'modine', '.', 'while', 'this', 'movie', 'has', 'lots', 'going', 'for', 'it', '(', 'a', 'very', 'amusing', 'script', 'offbeat', 'characters', 'sudden', 'sharp', 'turns', 'to', 'unexpected', 'violence', ',', 'as', 'in', 'director', 'jonathan', 'demme', \"'\", 's', 'previous', 'effort', 'something', 'wild', 'and', 'hilarious', 'yet', 'menacing', 'performances', 'by', 'stockwell', 'and', 'mercedes', 'ruehl', ',', 'as', 'his', 'jealous', 'wife', 'from', 'hell', ')', ',', 'michelle', 'steals', 'the', 'show', 'easily', '.', 'notice', 'how', 'perfectly', 'she', 'nails', 'angela', \"'\", 's', 'undereducated', ',', 'long', 'island', 'italian', 'accent', ',', 'and', 'the', 'many', 'fine', 'mannerisms', 'that', 'she', 'brings', 'to', 'the', 'role', 'to', 'really', 'flesh', 'out', 'this', 'spunky', 'and', 'surprisingly', 'bright', 'character', '.', 'once', 'upon', 'a', 'time', ',', 'long', 'ago', ',', 'oscars', 'were', 'handed', 'out', 'to', 'actresses', 'for', 'comedic', 'roles', 'such', 'as', 'this', 'one', '.', 'had', 'this', 'film', 'been', 'made', '60', 'years', 'ago', ',', 'michelle', 'mighta', 'been', 'a', 'contenduh', '.', '.', '.']\n",
            "Tokens: [56, 615, 9, 13095, 9103, 13, 3, 4935, 6, 8939, 5242, 13, 3, 10, 4339, 92, 467, 9, 196, 286, 22, 9905, 558, 2892, 5356, 13, 3, 10, 1725, 53, 1850, 59, 314, 204, 13, 3, 105, 8, 53, 118, 577, 603, 19, 42, 3708, 5, 3704, 1042, 558, 4, 245, 277, 22, 26604, 8, 5356, 10, 17, 1032, 3404, 361, 170, 65, 1091, 82, 53, 175, 13, 6819, 10, 17, 3129, 6, 1718, 48, 10, 15657, 10, 17, 120, 1838, 27, 7, 894, 523, 8, 1661, 25, 153, 277, 9, 70, 56, 7, 4892, 1749, 558, 63, 58, 35, 5, 60, 374, 3, 214, 181, 5, 151, 803, 52, 5481, 10, 17, 1099, 9, 3, 2543, 4, 13, 15, 36, 5, 63, 310, 3039, 63266, 5, 3, 4571, 8, 7, 1004, 36167, 2543, 7265, 5, 43, 1131, 44, 53, 35714, 5315, 210, 785, 352, 9, 375, 7, 184, 130, 22, 847, 6, 53, 494, 5, 146, 118, 6101, 41, 2543, 1482, 2244, 9566, 6, 2615, 135, 3467, 11803, 4, 146, 15, 20, 55, 708, 174, 22, 12, 27, 7, 62, 1140, 238, 8037, 111, 2469, 2179, 519, 9, 2130, 592, 5, 19, 13, 177, 2553, 8426, 10, 17, 886, 773, 147, 1403, 6, 537, 250, 4105, 382, 41, 9566, 6, 8469, 15500, 5, 19, 33, 3781, 339, 44, 578, 25, 5, 2892, 2140, 3, 124, 767, 4, 1317, 94, 883, 63, 5952, 3039, 10, 17, 39144, 5, 210, 785, 996, 1194, 5, 6, 3, 116, 499, 6861, 16, 63, 975, 9, 3, 221, 9, 72, 2262, 52, 15, 13440, 6, 1244, 1884, 115, 4, 293, 651, 7, 69, 5, 210, 601, 5, 4135, 78, 4548, 52, 9, 1657, 22, 1749, 618, 149, 19, 15, 36, 4, 76, 15, 24, 86, 106, 1924, 166, 601, 5, 2892, 78066, 86, 7, 61601, 4, 4, 4]\n",
            "\n",
            "Label: 1\n",
            "Text: This is a thoughtful film that lays bare the inequities of the so-called upper class and those who work for them, the haves and have-nots. Robert Shaw does a creditable job in his role as the obliging, correct chauffeur, Steven Ledbetter, who helps Lady Franklin (Sarah Miles) overcome her mental depression at the outset. However, Steven has many mixed feelings regarding this lady of the upper class. He inevitably falls in love with her, which of course is overstepping the societal boundaries that separate them.<br /><br />I have not read anything prior to this and only judge the movie as I have seen it. I consider it a very honest story about the realities of daily living and the conflict of what we might wish or expect from life and what we get. It's a fine drama worth seeing again.\n",
            "Split: ['this', 'is', 'a', 'thoughtful', 'film', 'that', 'lays', 'bare', 'the', 'inequities', 'of', 'the', 'so-called', 'upper', 'class', 'and', 'those', 'who', 'work', 'for', 'them', ',', 'the', 'haves', 'and', 'have-nots', '.', 'robert', 'shaw', 'does', 'a', 'creditable', 'job', 'in', 'his', 'role', 'as', 'the', 'obliging', ',', 'correct', 'chauffeur', ',', 'steven', 'ledbetter', ',', 'who', 'helps', 'lady', 'franklin', '(', 'sarah', 'miles', ')', 'overcome', 'her', 'mental', 'depression', 'at', 'the', 'outset', '.', 'however', ',', 'steven', 'has', 'many', 'mixed', 'feelings', 'regarding', 'this', 'lady', 'of', 'the', 'upper', 'class', '.', 'he', 'inevitably', 'falls', 'in', 'love', 'with', 'her', ',', 'which', 'of', 'course', 'is', 'overstepping', 'the', 'societal', 'boundaries', 'that', 'separate', 'them', '.', 'i', 'have', 'not', 'read', 'anything', 'prior', 'to', 'this', 'and', 'only', 'judge', 'the', 'movie', 'as', 'i', 'have', 'seen', 'it', '.', 'i', 'consider', 'it', 'a', 'very', 'honest', 'story', 'about', 'the', 'realities', 'of', 'daily', 'living', 'and', 'the', 'conflict', 'of', 'what', 'we', 'might', 'wish', 'or', 'expect', 'from', 'life', 'and', 'what', 'we', 'get', '.', 'it', \"'\", 's', 'a', 'fine', 'drama', 'worth', 'seeing', 'again', '.']\n",
            "Tokens: [15, 11, 7, 4380, 24, 16, 8319, 3281, 3, 31366, 8, 3, 3239, 4087, 823, 6, 153, 43, 175, 22, 100, 5, 3, 27874, 6, 44335, 4, 560, 5904, 133, 7, 17301, 304, 13, 33, 221, 19, 3, 32032, 5, 2190, 18103, 5, 2055, 21985, 5, 43, 1495, 838, 9100, 27, 2717, 2064, 25, 3442, 53, 1964, 4300, 38, 3, 7662, 4, 198, 5, 2055, 55, 116, 2011, 1332, 2544, 15, 838, 8, 3, 4087, 823, 4, 32, 5561, 771, 13, 120, 21, 53, 5, 73, 8, 272, 11, 47093, 3, 11563, 9055, 16, 2642, 100, 4, 14, 34, 30, 345, 242, 2496, 9, 15, 6, 74, 1470, 3, 20, 19, 14, 34, 117, 12, 4, 14, 1210, 12, 7, 62, 1145, 75, 51, 3, 7674, 8, 2738, 593, 6, 3, 1799, 8, 56, 79, 237, 623, 48, 513, 44, 130, 6, 56, 79, 85, 4, 12, 10, 17, 7, 499, 471, 271, 316, 180, 4]\n",
            "\n",
            "Number of classes: 2\n",
            "Number of samples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0eLPLiXRWKv",
        "outputId": "411cf7e7-285d-4ff5-a48b-791ad2532ff7"
      },
      "source": [
        "VALID_DATASET_PATH = '/content/gdrive/MyDrive/Datasets/Text/IMDB_validation_dataset.csv'\n",
        "TEST_DATASET_PATH = '/content/gdrive/MyDrive/Datasets/Text/IMDB_test_with_summary_dataset.csv'\n",
        "\n",
        "valid_dataset = DataFrameDataset(csv_file_path=VALID_DATASET_PATH, only_columns=['label', 'text'])\n",
        "test_dataset = DataFrameDataset(csv_file_path=TEST_DATASET_PATH, only_columns=['label', 'summary'])\n",
        "full_test_dataset = DataFrameDataset(csv_file_path=TEST_DATASET_PATH, only_columns=['label', 'text', 'summary'])\n",
        "print(f'Validation dataset size is: {len(valid_dataset)}')\n",
        "print(f'Test dataset size is: {len(test_dataset)}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation dataset size is: 16434\n",
            "Test dataset size is: 8467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M3277lZSXXJ",
        "outputId": "3a8dcf9e-3e3b-4442-96e0-9daef732b56d"
      },
      "source": [
        "test_loader = DataLoader(full_test_dataset, batch_size=2, shuffle=False)\n",
        "for batch in test_loader:\n",
        "  print('Print samples from a single batch:\\n')\n",
        "  labels, texts, summaries = batch\n",
        "  for i in range(len(labels)):\n",
        "    print(f'Sample: {i}')\n",
        "    print(f'Label: {labels[i]}')\n",
        "    print(f'Text: {texts[i]}')\n",
        "    print(f'Label: {summaries[i]}')\n",
        "    print('\\n')\n",
        "  break"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Print samples from a single batch:\n",
            "\n",
            "Sample: 0\n",
            "Label: neg\n",
            "Text: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to maki\n",
            "Label: I AM CURIOUS-YELLOW is a film about a young Swedish drama student named Lena who wants to learn everything she can about life. The plot is centered around a maki girl named Maki who wants to focus her attentions on the maki boy's maki, which is a game of maki. The film was released in 1967 and has been rated 4/5 (Sweden).\n",
            "\n",
            "\n",
            "Sample: 1\n",
            "Label: neg\n",
            "Text: \"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swing\n",
            "Label: \"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. Where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFw4dSbJWODO"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device) "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivnGdjNMWOFz"
      },
      "source": [
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7Ms8i1-WOIJ"
      },
      "source": [
        "train_iter = IMDB(split='test')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR7TagKqWOK1"
      },
      "source": [
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text, offsets)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predited_label = model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IkN1G6mWONN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9b4af0-6f2b-4645-f14c-0d98c43ef896"
      },
      "source": [
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 32 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter = IMDB(split='test')\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "# test_dataset = to_map_style_dataset(test_iter)\n",
        "# num_train = int(len(train_dataset) * 0.95)\n",
        "# split_train_, split_valid_ = \\\n",
        "#     random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "      total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/  782 batches | accuracy    0.585\n",
            "| epoch   1 |   400/  782 batches | accuracy    0.694\n",
            "| epoch   1 |   600/  782 batches | accuracy    0.755\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 14.05s | valid accuracy    0.643 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   200/  782 batches | accuracy    0.800\n",
            "| epoch   2 |   400/  782 batches | accuracy    0.809\n",
            "| epoch   2 |   600/  782 batches | accuracy    0.814\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 13.73s | valid accuracy    0.762 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   200/  782 batches | accuracy    0.836\n",
            "| epoch   3 |   400/  782 batches | accuracy    0.847\n",
            "| epoch   3 |   600/  782 batches | accuracy    0.842\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 13.98s | valid accuracy    0.780 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   200/  782 batches | accuracy    0.848\n",
            "| epoch   4 |   400/  782 batches | accuracy    0.859\n",
            "| epoch   4 |   600/  782 batches | accuracy    0.859\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 13.65s | valid accuracy    0.793 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   200/  782 batches | accuracy    0.867\n",
            "| epoch   5 |   400/  782 batches | accuracy    0.873\n",
            "| epoch   5 |   600/  782 batches | accuracy    0.864\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 13.67s | valid accuracy    0.805 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   200/  782 batches | accuracy    0.880\n",
            "| epoch   6 |   400/  782 batches | accuracy    0.875\n",
            "| epoch   6 |   600/  782 batches | accuracy    0.874\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 13.69s | valid accuracy    0.779 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   200/  782 batches | accuracy    0.905\n",
            "| epoch   7 |   400/  782 batches | accuracy    0.911\n",
            "| epoch   7 |   600/  782 batches | accuracy    0.914\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 13.71s | valid accuracy    0.796 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   200/  782 batches | accuracy    0.921\n",
            "| epoch   8 |   400/  782 batches | accuracy    0.906\n",
            "| epoch   8 |   600/  782 batches | accuracy    0.920\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 13.73s | valid accuracy    0.798 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   200/  782 batches | accuracy    0.916\n",
            "| epoch   9 |   400/  782 batches | accuracy    0.922\n",
            "| epoch   9 |   600/  782 batches | accuracy    0.913\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 13.54s | valid accuracy    0.799 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   200/  782 batches | accuracy    0.915\n",
            "| epoch  10 |   400/  782 batches | accuracy    0.917\n",
            "| epoch  10 |   600/  782 batches | accuracy    0.917\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 13.73s | valid accuracy    0.799 \n",
            "-----------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4wj_dhQWOPL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b29e78f-1848-4a8a-a935-65ebf15d3462"
      },
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZiqQEhbWjti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b3b8c2-e63c-4466-a9d8-5af74869ea2a"
      },
      "source": [
        "imdb_label = {0: 'negative',\n",
        "              1: 'positive'}\n",
        "\n",
        "def predict(text, text_pipeline):\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor(text_pipeline(text))\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item()\n",
        "\n",
        "ex_positive_str = \"Full of suspense, gripping the entire time, intense, \\\n",
        "  two stories in parallel that come together, somewhat predictable betrayals \\\n",
        "   and twist, felt a bit dark at times, satisfying ending, not a huge amount \\\n",
        "    of action but a solid storyline that keeps you on edge.\"\n",
        "\n",
        "ex_negative_str = \"This is not your traditional Guy Ritchie movie with slick \\\n",
        "   fast paced action, clever humour and lots of twists. Which I have loved in \\\n",
        "    the past. It is basically a combination of heist movie and revenge \\\n",
        "     thriller. But it's played very straight, without a lot of effort to \\\n",
        "      build characters, and doesn't ever seem to build much momentum. So \\\n",
        "       a few times during the movie I found myself looking at my watch, \\\n",
        "        wondering if it was really going anywhere. The action is fairly \\\n",
        "         tight but mainly gunplay, not much physical action as Statham is \\\n",
        "          famous for. There are no heroes either, Stathams character seems  \\\n",
        "          to be a pretty nasty piece of work himself. All in all, it's an \\\n",
        "           average thriller with nothing in particular to recommend it.\"\n",
        "\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s review\" %imdb_label[predict(ex_negative_str, text_pipeline)])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a negative news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7DN6NobWjwJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}