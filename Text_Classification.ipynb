{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOmQgib/+dizWXYKUN0WDUL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoyElkabetz/Text-Summarization-with-Deep-Learning/blob/main/Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F96lfuaZWBI6",
        "outputId": "6b7eb03f-da8c-49e0-fdcb-4e5825ba5ba5"
      },
      "source": [
        "%matplotlib inline\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f'torch {torch.__version__}')\n",
        "print('Device properties:')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    gpu_data = torch.cuda.get_device_properties(0)\n",
        "    gpu_name = gpu_data.name\n",
        "    gpu_mem  = f'{gpu_data.total_memory * 1e-9:.02f} Gb'\n",
        "    print(f'GPU: {gpu_name}\\nMemory: {gpu_mem}')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('CPU')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch 1.9.0+cu102\n",
            "Device properties:\n",
            "CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fQONvlkWN7y",
        "outputId": "d9c00b59-c2ba-4473-a37a-5521a908d577"
      },
      "source": [
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = AG_NEWS(split='train')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "vocab(['my', 'name', 'is', 'Roy', '!'])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.csv: 29.5MB [00:00, 75.3MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1300, 951, 21, 0, 764]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ModijEJY714"
      },
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AoszQTWWN-a",
        "outputId": "cbbbef4c-4a17-497c-d258-3e1b5a4c7c81"
      },
      "source": [
        "train_iter = AG_NEWS(split='train')\n",
        "n_samples = len(train_iter)\n",
        "random_list = torch.randint(0, n_samples - 1, (4, ))\n",
        "labels = []\n",
        "for i, (label, text) in enumerate(train_iter):\n",
        "    labels.append(label)\n",
        "    if i in random_list:\n",
        "        print(f'Label: {label_pipeline(label)}')\n",
        "        print(f'Text: {text}')\n",
        "        print(f'Split: {tokenizer(text)}')\n",
        "        print(f'Tokens: {text_pipeline(text)}\\n')\n",
        "print('Number of classes: {}'.format(len(set(labels))))\n",
        "print('Number of samples: {}'.format(n_samples))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 1\n",
            "Text: PGA Changes Dates of Grand Slam of Golf (AP) AP - The Grand Slam of Golf, which brings together the PGA Tour's four major champions, has been switched to Thanksgiving week.\n",
            "Split: ['pga', 'changes', 'dates', 'of', 'grand', 'slam', 'of', 'golf', '(', 'ap', ')', 'ap', '-', 'the', 'grand', 'slam', 'of', 'golf', ',', 'which', 'brings', 'together', 'the', 'pga', 'tour', \"'\", 's', 'four', 'major', 'champions', ',', 'has', 'been', 'switched', 'to', 'thanksgiving', 'week', '.']\n",
            "Tokens: [2076, 1125, 7627, 6, 589, 2622, 6, 1114, 13, 31, 14, 31, 15, 2, 589, 2622, 6, 1114, 3, 103, 2739, 1518, 2, 2076, 712, 16, 9, 161, 197, 544, 3, 28, 81, 11306, 4, 3579, 85, 1]\n",
            "\n",
            "Label: 0\n",
            "Text: U.S. says UN, Africa and Sudan itself hold solution to Darfur crisis (Canadian Press) Canadian Press - WASHINGTON (AP) - The key to stopping the ethnic violence in western Sudan rests not with the United States alone but with the United Nations, other African countries and Sudan itself, President George W. Bush's foreign policy advisers said Sunday.\n",
            "Split: ['u', '.', 's', '.', 'says', 'un', ',', 'africa', 'and', 'sudan', 'itself', 'hold', 'solution', 'to', 'darfur', 'crisis', '(', 'canadian', 'press', ')', 'canadian', 'press', '-', 'washington', '(', 'ap', ')', '-', 'the', 'key', 'to', 'stopping', 'the', 'ethnic', 'violence', 'in', 'western', 'sudan', 'rests', 'not', 'with', 'the', 'united', 'states', 'alone', 'but', 'with', 'the', 'united', 'nations', ',', 'other', 'african', 'countries', 'and', 'sudan', 'itself', ',', 'president', 'george', 'w', '.', 'bush', \"'\", 's', 'foreign', 'policy', 'advisers', 'said', 'sunday', '.']\n",
            "Tokens: [51, 1, 9, 1, 84, 286, 3, 699, 8, 542, 1582, 662, 2223, 4, 464, 870, 13, 364, 328, 14, 364, 328, 15, 144, 13, 31, 14, 15, 2, 372, 4, 6436, 2, 4048, 618, 7, 830, 542, 16860, 62, 18, 2, 88, 159, 3900, 45, 18, 2, 88, 410, 3, 158, 788, 776, 8, 542, 1582, 3, 76, 833, 1090, 1, 128, 16, 9, 396, 918, 6205, 26, 91, 1]\n",
            "\n",
            "Label: 3\n",
            "Text: Office Workers, Move Over: Here Comes Honda's Asimo  WAKO, Japan (Reuters) - Honda Motor Co.'s humanoid robot  has been around: he's rung the famed bell at the New York Stock  Exchange, met Spain's king, and even traveled with Japan's  prime minister to Prague as a goodwill ambassador.\n",
            "Split: ['office', 'workers', ',', 'move', 'over', 'here', 'comes', 'honda', \"'\", 's', 'asimo', 'wako', ',', 'japan', '(', 'reuters', ')', '-', 'honda', 'motor', 'co', '.', \"'\", 's', 'humanoid', 'robot', 'has', 'been', 'around', 'he', \"'\", 's', 'rung', 'the', 'famed', 'bell', 'at', 'the', 'new', 'york', 'stock', 'exchange', ',', 'met', 'spain', \"'\", 's', 'king', ',', 'and', 'even', 'traveled', 'with', 'japan', \"'\", 's', 'prime', 'minister', 'to', 'prague', 'as', 'a', 'goodwill', 'ambassador', '.']\n",
            "Tokens: [536, 400, 3, 284, 38, 475, 1100, 3366, 16, 9, 42748, 94063, 3, 193, 13, 27, 14, 15, 3366, 1570, 218, 1, 16, 9, 32330, 3867, 28, 81, 435, 49, 16, 9, 29638, 2, 8769, 2045, 20, 2, 23, 73, 294, 687, 3, 1422, 728, 16, 9, 1333, 3, 8, 331, 9140, 18, 193, 16, 9, 199, 125, 4, 8825, 19, 5, 9492, 3802, 1]\n",
            "\n",
            "Label: 3\n",
            "Text: Ill. Gov. Seeks Ban on Violent Video Games (AP) AP - Gov. Rod Blagojevich is proposing to make it a misdemeanor for businesses to sell violent and sexually explicit video games to minors, a step that other states have tried with little success.\n",
            "Split: ['ill', '.', 'gov', '.', 'seeks', 'ban', 'on', 'violent', 'video', 'games', '(', 'ap', ')', 'ap', '-', 'gov', '.', 'rod', 'blagojevich', 'is', 'proposing', 'to', 'make', 'it', 'a', 'misdemeanor', 'for', 'businesses', 'to', 'sell', 'violent', 'and', 'sexually', 'explicit', 'video', 'games', 'to', 'minors', ',', 'a', 'step', 'that', 'other', 'states', 'have', 'tried', 'with', 'little', 'success', '.']\n",
            "Tokens: [3182, 1, 2695, 1, 1001, 867, 10, 2402, 398, 221, 13, 31, 14, 31, 15, 2695, 1, 6021, 13868, 21, 8969, 4, 203, 25, 5, 16089, 11, 985, 4, 562, 2402, 8, 6632, 9485, 398, 221, 4, 8949, 3, 5, 727, 17, 158, 159, 39, 1782, 18, 548, 1331, 1]\n",
            "\n",
            "Number of classes: 4\n",
            "Number of samples: 120000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loobLmVTYp0F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wvPp_DGWOA9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFw4dSbJWODO"
      },
      "source": [
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)    \n",
        "\n",
        "train_iter = AG_NEWS(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivnGdjNMWOFz"
      },
      "source": [
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7Ms8i1-WOIJ"
      },
      "source": [
        "train_iter = AG_NEWS(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR7TagKqWOK1"
      },
      "source": [
        "\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text, offsets)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predited_label = model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IkN1G6mWONN"
      },
      "source": [
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4wj_dhQWOPL"
      },
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZiqQEhbWjti"
      },
      "source": [
        "ag_news_label = {1: \"World\",\n",
        "                 2: \"Sports\",\n",
        "                 3: \"Business\",\n",
        "                 4: \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, text_pipeline):\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor(text_pipeline(text))\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7DN6NobWjwJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}