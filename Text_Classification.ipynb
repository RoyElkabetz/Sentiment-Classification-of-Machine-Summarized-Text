{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPj9JrfAwkI0RJgMrLFUUGu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoyElkabetz/Text-Summarization-with-Deep-Learning/blob/main/Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KMfu5tAbJrz",
        "outputId": "c600c4e3-a5e9-4f66-89df-17ed32a9d815"
      },
      "source": [
        "## uncomment only if running from google.colab\n",
        "# clone the git reposetory\n",
        "!git clone https://github.com/RoyElkabetz/Text-Summarization-with-Deep-Learning\n",
        "# add path to .py files for import\n",
        "import sys\n",
        "sys.path.insert(1, \"/content/Text-Summarization-with-Deep-Learning\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Text-Summarization-with-Deep-Learning'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 53 (delta 28), reused 6 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (53/53), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBIVxaAlSFD3",
        "outputId": "77c08ce5-c6fc-45be-d730-655103ae5484"
      },
      "source": [
        "## uncomment if you want to mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F96lfuaZWBI6",
        "outputId": "6c581576-e951-4efc-f65a-7dd5a5611c9c"
      },
      "source": [
        "%matplotlib inline\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torchtext.datasets import IMDB\n",
        "import torchtext.data as data\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f'torch {torch.__version__}')\n",
        "print('Device properties:')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    gpu_data = torch.cuda.get_device_properties(0)\n",
        "    gpu_name = gpu_data.name\n",
        "    gpu_mem  = f'{gpu_data.total_memory * 1e-9:.02f} Gb'\n",
        "    print(f'GPU: {gpu_name}\\nMemory: {gpu_mem}')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('CPU')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch 1.9.0+cu102\n",
            "Device properties:\n",
            "CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj1U-8ysnXbA"
      },
      "source": [
        "class DataFrameDataset(Dataset):\n",
        "  \"\"\"Create a torch.utils.data.Dataset from a pandas.DataFrame or a CSV file.\"\"\"\n",
        "\n",
        "  def __init__(self, csv_file_path=None, pd_dataframe=None, only_columns=None):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "      csv_file_path (string): Path to the csv file with annotations.\n",
        "      pd_dataframe (Pandas DataFrame): A Pandas DataFrame with containing the\n",
        "      data.\n",
        "      only_columns (list): A List of colums names from the data. \n",
        "    \"\"\"\n",
        "    if isinstance(pd_dataframe, pd.DataFrame):\n",
        "      self.df = pd_dataframe \n",
        "    else:\n",
        "      self.df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    if only_columns is not None:\n",
        "      if isinstance(only_columns, list):\n",
        "        for item in only_columns:\n",
        "          if item not in self.df.columns:\n",
        "            raise ValueError(f\"Got a column name '{item}' in only_columns which is not in DataFrame columns.\")\n",
        "        self.only_columns = only_columns\n",
        "      else:\n",
        "        raise TypeError(f\"only_columns must be a <class 'list'>, instead got a {type(only_columns)}.\")\n",
        "    else:\n",
        "      self.only_columns = list(self.df.columns)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    row = self.df.iloc[idx][self.only_columns]\n",
        "    row_list = [item for item in row]\n",
        "    return row_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD5sdelTP2ex"
      },
      "source": [
        "## Get the IMDB dataset and create a vocabulary from the train dataset\n",
        "I use the IMDB test data as train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fQONvlkWN7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "784d09d9-ad8e-4d7c-ffa1-61fbb3302e1e"
      },
      "source": [
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = IMDB(split='test')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\", \"<sos>\", \"<eos>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 70.0MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjck56H3QpuU"
      },
      "source": [
        "## Create text and labels pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ModijEJY714"
      },
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: 0 if x=='neg' else 1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ8CR_ktQx1G"
      },
      "source": [
        "## Print some random samples and the size of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AoszQTWWN-a",
        "outputId": "1edd24f0-0303-4318-b47e-a19c732029c0"
      },
      "source": [
        "train_iter = IMDB(split='test')\n",
        "n_samples = len(train_iter)\n",
        "random_list = torch.randint(0, n_samples - 1, (4, ))\n",
        "labels = []\n",
        "for i, (label, text) in enumerate(train_iter):\n",
        "    labels.append(label)\n",
        "    if i in random_list:\n",
        "        print(f'Label: {label_pipeline(label)}')\n",
        "        print(f'Text: {text}')\n",
        "        print(f'Split: {tokenizer(text)}')\n",
        "        print(f'Tokens: {text_pipeline(text)}\\n')\n",
        "print('Number of classes: {}'.format(len(set(labels))))\n",
        "print('Number of samples: {}'.format(n_samples))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 0\n",
            "Text: I say \"flick\" because this doesn't deserve the appellation \"movie\", and certainly not \"film\". I regret paying for the rental, and although I've never walked out on a movie before, this would have been it, had I seen it in a theatre. A society living underground in the future (oooh, THAT'S original), lots of burning barrel drums, unexplained ambient light shining through windows, an ungrateful woman and her shock-muted son...the list goes on and on. C. Thomas Howell affects the husky voice of the stereotypical loner; you know like Eastwood's been done to death. He needs special sunglasses to remember his wife and child, yet in the flashbacks, he's the same age! Talk about a poor memory! I stared incredulously when the little boy Abe randomly pushes a code into a door and it opens! No tension, pithy religious (what religion?) under/overtones...saddest of all: I expected better from Roddy Piper;<br /><br />Quite possibly the worst movie experience in my life.\n",
            "Split: ['i', 'say', 'flick', 'because', 'this', 'doesn', \"'\", 't', 'deserve', 'the', 'appellation', 'movie', ',', 'and', 'certainly', 'not', 'film', '.', 'i', 'regret', 'paying', 'for', 'the', 'rental', ',', 'and', 'although', 'i', \"'\", 've', 'never', 'walked', 'out', 'on', 'a', 'movie', 'before', ',', 'this', 'would', 'have', 'been', 'it', ',', 'had', 'i', 'seen', 'it', 'in', 'a', 'theatre', '.', 'a', 'society', 'living', 'underground', 'in', 'the', 'future', '(', 'oooh', ',', 'that', \"'\", 's', 'original', ')', ',', 'lots', 'of', 'burning', 'barrel', 'drums', ',', 'unexplained', 'ambient', 'light', 'shining', 'through', 'windows', ',', 'an', 'ungrateful', 'woman', 'and', 'her', 'shock-muted', 'son', '.', '.', '.', 'the', 'list', 'goes', 'on', 'and', 'on', '.', 'c', '.', 'thomas', 'howell', 'affects', 'the', 'husky', 'voice', 'of', 'the', 'stereotypical', 'loner', 'you', 'know', 'like', 'eastwood', \"'\", 's', 'been', 'done', 'to', 'death', '.', 'he', 'needs', 'special', 'sunglasses', 'to', 'remember', 'his', 'wife', 'and', 'child', ',', 'yet', 'in', 'the', 'flashbacks', ',', 'he', \"'\", 's', 'the', 'same', 'age', '!', 'talk', 'about', 'a', 'poor', 'memory', '!', 'i', 'stared', 'incredulously', 'when', 'the', 'little', 'boy', 'abe', 'randomly', 'pushes', 'a', 'code', 'into', 'a', 'door', 'and', 'it', 'opens', '!', 'no', 'tension', ',', 'pithy', 'religious', '(', 'what', 'religion', '?', ')', 'under/overtones', '.', '.', '.', 'saddest', 'of', 'all', 'i', 'expected', 'better', 'from', 'roddy', 'piper', 'quite', 'possibly', 'the', 'worst', 'movie', 'experience', 'in', 'my', 'life', '.']\n",
            "Tokens: [14, 139, 491, 97, 15, 163, 10, 29, 1647, 3, 55442, 20, 5, 6, 420, 30, 24, 4, 14, 2456, 2715, 22, 3, 2147, 5, 6, 266, 14, 10, 148, 119, 2044, 52, 28, 7, 20, 167, 5, 15, 68, 34, 86, 12, 5, 76, 14, 117, 12, 13, 7, 1607, 4, 7, 843, 593, 2538, 13, 3, 766, 27, 16792, 5, 16, 10, 17, 230, 25, 5, 708, 8, 3771, 6292, 13204, 5, 5829, 14544, 720, 4520, 154, 5977, 5, 42, 17060, 263, 6, 53, 89044, 494, 4, 4, 4, 3, 1047, 284, 28, 6, 28, 4, 1191, 4, 1360, 9659, 6910, 3, 13282, 613, 8, 3, 2381, 9676, 26, 123, 46, 2080, 10, 17, 86, 228, 9, 328, 4, 32, 762, 313, 13454, 9, 393, 33, 339, 6, 555, 5, 250, 13, 3, 2019, 5, 32, 10, 17, 3, 176, 688, 37, 691, 51, 7, 336, 1601, 37, 14, 13002, 36229, 60, 3, 129, 428, 16331, 5702, 5106, 7, 2929, 93, 7, 1265, 6, 12, 2166, 37, 65, 1106, 5, 17680, 1692, 27, 56, 2366, 54, 25, 95156, 4, 4, 4, 8644, 8, 39, 14, 835, 134, 44, 15494, 4363, 192, 911, 3, 256, 20, 533, 13, 64, 130, 4]\n",
            "\n",
            "Label: 0\n",
            "Text: One of the worst movies I've ever seen. Completely ridiculous. The story is bad. The animations are completely childish and displaced. The physics of the holograms are hilarious from how much they are completely wrong. OMG, I even wanna believe that this film has Disney label on it. Yuck. The actresses are somewhat beautiful, but there are so many good films with astonishing actresses that are far more valuable to see than this one. Final remark, bad film. Don't bother to watch it. If you're looking for films to see with your kids consider other alternatives like Ratattuile, Monster Inc or an Enchanted Story (on theaters). Seriously.\n",
            "Split: ['one', 'of', 'the', 'worst', 'movies', 'i', \"'\", 've', 'ever', 'seen', '.', 'completely', 'ridiculous', '.', 'the', 'story', 'is', 'bad', '.', 'the', 'animations', 'are', 'completely', 'childish', 'and', 'displaced', '.', 'the', 'physics', 'of', 'the', 'holograms', 'are', 'hilarious', 'from', 'how', 'much', 'they', 'are', 'completely', 'wrong', '.', 'omg', ',', 'i', 'even', 'wanna', 'believe', 'that', 'this', 'film', 'has', 'disney', 'label', 'on', 'it', '.', 'yuck', '.', 'the', 'actresses', 'are', 'somewhat', 'beautiful', ',', 'but', 'there', 'are', 'so', 'many', 'good', 'films', 'with', 'astonishing', 'actresses', 'that', 'are', 'far', 'more', 'valuable', 'to', 'see', 'than', 'this', 'one', '.', 'final', 'remark', ',', 'bad', 'film', '.', 'don', \"'\", 't', 'bother', 'to', 'watch', 'it', '.', 'if', 'you', \"'\", 're', 'looking', 'for', 'films', 'to', 'see', 'with', 'your', 'kids', 'consider', 'other', 'alternatives', 'like', 'ratattuile', ',', 'monster', 'inc', 'or', 'an', 'enchanted', 'story', '(', 'on', 'theaters', ')', '.', 'seriously', '.']\n",
            "Tokens: [36, 8, 3, 256, 107, 14, 10, 148, 131, 117, 4, 334, 654, 4, 3, 75, 11, 88, 4, 3, 11942, 31, 334, 4649, 6, 14128, 4, 3, 7669, 8, 3, 71738, 31, 537, 44, 94, 81, 40, 31, 334, 360, 4, 10398, 5, 14, 67, 3224, 265, 16, 15, 24, 55, 1121, 5630, 28, 12, 4, 13059, 4, 3, 1657, 31, 693, 319, 5, 23, 47, 31, 45, 116, 59, 114, 21, 4715, 1657, 16, 31, 232, 61, 4981, 9, 70, 82, 15, 36, 4, 464, 8805, 5, 88, 24, 4, 95, 10, 29, 1108, 9, 113, 12, 4, 50, 26, 10, 183, 277, 22, 114, 9, 70, 21, 132, 348, 1210, 84, 21282, 46, 85433, 5, 652, 14771, 48, 42, 8900, 75, 27, 28, 2111, 25, 4, 603, 4]\n",
            "\n",
            "Label: 1\n",
            "Text: The role of economics in the industrialized North American market must have always been theorized in the homelands of the engines creation. Persons and industrialist such as Mercedes Benz and the Bavarian Motor Works (BMW) surely realized the opportunity of the North Ameircan market with the purchase of fuel and number of automobiles purchased per household. This type of economic phenomena sparked the concept of Speed Racer.<br /><br />After the new constitution of Japan the industrialization of the isolated island nation of Japan must seek opportunity once again via economic partnerships with its global neighbors. This also helped spark the economic opportunities in the European and North American market if not the global market.<br /><br />Speed is a young avid driver who without knowing any better is driven by his demanding father Pops Racer who has challenged himself his whole life to make a better machine better at winning races. It was in fact Pops Racer who drove his first son Rex Racer to the brink of destruction with his strategy of how to best use the technology he developed. As a mature Racer, Rex, finally realizes his own inherent values and becomes independent but still feels obligated to his younger bother Speed.<br /><br />The exact relationship of Rex Racer to persons such as the Inspector are never really clear, but put into dramatization. Rex is eventually accused of being a type of agent for a country or organization due to his ability to be in places at times when there is no other explanation to how he would have known Speed was in trouble. Or the fact that the situations involved some types of illegal activity were his secretive knowledge is leveraged against an evil plot. This brings a level of cloak and dagger romance to Speed Racer.<br /><br />The mixture of Speeds innocence with Trixy, Sprital, and Chim Chim brings a level of comic human nature. This concept is a good form of rhetoric to balance the themes and plots as they are played out from episode to episode. So, instead of a dry detective story the thrill of international race car driving, romance of cloak and dagger, and comedy of human nature is put into one story, Speed Racer.\n",
            "Split: ['the', 'role', 'of', 'economics', 'in', 'the', 'industrialized', 'north', 'american', 'market', 'must', 'have', 'always', 'been', 'theorized', 'in', 'the', 'homelands', 'of', 'the', 'engines', 'creation', '.', 'persons', 'and', 'industrialist', 'such', 'as', 'mercedes', 'benz', 'and', 'the', 'bavarian', 'motor', 'works', '(', 'bmw', ')', 'surely', 'realized', 'the', 'opportunity', 'of', 'the', 'north', 'ameircan', 'market', 'with', 'the', 'purchase', 'of', 'fuel', 'and', 'number', 'of', 'automobiles', 'purchased', 'per', 'household', '.', 'this', 'type', 'of', 'economic', 'phenomena', 'sparked', 'the', 'concept', 'of', 'speed', 'racer', '.', 'after', 'the', 'new', 'constitution', 'of', 'japan', 'the', 'industrialization', 'of', 'the', 'isolated', 'island', 'nation', 'of', 'japan', 'must', 'seek', 'opportunity', 'once', 'again', 'via', 'economic', 'partnerships', 'with', 'its', 'global', 'neighbors', '.', 'this', 'also', 'helped', 'spark', 'the', 'economic', 'opportunities', 'in', 'the', 'european', 'and', 'north', 'american', 'market', 'if', 'not', 'the', 'global', 'market', '.', 'speed', 'is', 'a', 'young', 'avid', 'driver', 'who', 'without', 'knowing', 'any', 'better', 'is', 'driven', 'by', 'his', 'demanding', 'father', 'pops', 'racer', 'who', 'has', 'challenged', 'himself', 'his', 'whole', 'life', 'to', 'make', 'a', 'better', 'machine', 'better', 'at', 'winning', 'races', '.', 'it', 'was', 'in', 'fact', 'pops', 'racer', 'who', 'drove', 'his', 'first', 'son', 'rex', 'racer', 'to', 'the', 'brink', 'of', 'destruction', 'with', 'his', 'strategy', 'of', 'how', 'to', 'best', 'use', 'the', 'technology', 'he', 'developed', '.', 'as', 'a', 'mature', 'racer', ',', 'rex', ',', 'finally', 'realizes', 'his', 'own', 'inherent', 'values', 'and', 'becomes', 'independent', 'but', 'still', 'feels', 'obligated', 'to', 'his', 'younger', 'bother', 'speed', '.', 'the', 'exact', 'relationship', 'of', 'rex', 'racer', 'to', 'persons', 'such', 'as', 'the', 'inspector', 'are', 'never', 'really', 'clear', ',', 'but', 'put', 'into', 'dramatization', '.', 'rex', 'is', 'eventually', 'accused', 'of', 'being', 'a', 'type', 'of', 'agent', 'for', 'a', 'country', 'or', 'organization', 'due', 'to', 'his', 'ability', 'to', 'be', 'in', 'places', 'at', 'times', 'when', 'there', 'is', 'no', 'other', 'explanation', 'to', 'how', 'he', 'would', 'have', 'known', 'speed', 'was', 'in', 'trouble', '.', 'or', 'the', 'fact', 'that', 'the', 'situations', 'involved', 'some', 'types', 'of', 'illegal', 'activity', 'were', 'his', 'secretive', 'knowledge', 'is', 'leveraged', 'against', 'an', 'evil', 'plot', '.', 'this', 'brings', 'a', 'level', 'of', 'cloak', 'and', 'dagger', 'romance', 'to', 'speed', 'racer', '.', 'the', 'mixture', 'of', 'speeds', 'innocence', 'with', 'trixy', ',', 'sprital', ',', 'and', 'chim', 'chim', 'brings', 'a', 'level', 'of', 'comic', 'human', 'nature', '.', 'this', 'concept', 'is', 'a', 'good', 'form', 'of', 'rhetoric', 'to', 'balance', 'the', 'themes', 'and', 'plots', 'as', 'they', 'are', 'played', 'out', 'from', 'episode', 'to', 'episode', '.', 'so', ',', 'instead', 'of', 'a', 'dry', 'detective', 'story', 'the', 'thrill', 'of', 'international', 'race', 'car', 'driving', ',', 'romance', 'of', 'cloak', 'and', 'dagger', ',', 'and', 'comedy', 'of', 'human', 'nature', 'is', 'put', 'into', 'one', 'story', ',', 'speed', 'racer', '.']\n",
            "Tokens: [3, 221, 8, 27516, 13, 3, 44847, 1983, 305, 2681, 217, 34, 222, 86, 93187, 13, 3, 44539, 8, 3, 10591, 2696, 4, 4735, 6, 25531, 149, 19, 8469, 57157, 6, 3, 21351, 10152, 511, 27, 21384, 25, 1444, 1643, 3, 1442, 8, 3, 1983, 54851, 2681, 21, 3, 4077, 8, 6670, 6, 634, 8, 14561, 4886, 3659, 4909, 4, 15, 567, 8, 7879, 9704, 16202, 3, 1152, 8, 2394, 10420, 4, 110, 3, 184, 13588, 8, 2617, 3, 72925, 8, 3, 3560, 785, 3441, 8, 2617, 217, 2749, 1442, 293, 180, 2488, 7879, 37519, 21, 101, 5680, 5245, 4, 15, 92, 1551, 5203, 3, 7879, 6426, 13, 3, 2081, 6, 1983, 305, 2681, 50, 30, 3, 5680, 2681, 4, 2394, 11, 7, 203, 5539, 1877, 43, 215, 1390, 109, 134, 11, 2614, 41, 33, 5227, 371, 4598, 10420, 43, 55, 4467, 324, 33, 224, 130, 9, 104, 7, 134, 1386, 134, 38, 2276, 5302, 4, 12, 18, 13, 206, 4598, 10420, 43, 5338, 33, 99, 494, 5517, 10420, 9, 3, 11640, 8, 3165, 21, 33, 9976, 8, 94, 9, 128, 357, 3, 2523, 32, 1540, 4, 19, 7, 2651, 10420, 5, 5517, 5, 433, 2627, 33, 209, 6765, 1148, 6, 487, 1689, 23, 143, 822, 15400, 9, 33, 1078, 1108, 2394, 4, 3, 2477, 624, 8, 5517, 10420, 9, 4735, 149, 19, 3, 4354, 31, 119, 72, 726, 5, 23, 279, 93, 11126, 4, 5517, 11, 841, 3350, 8, 118, 7, 567, 8, 1413, 22, 7, 644, 48, 6172, 687, 9, 33, 1201, 9, 35, 13, 1318, 38, 219, 60, 47, 11, 65, 84, 1709, 9, 94, 32, 68, 34, 594, 2394, 18, 13, 1101, 4, 48, 3, 206, 16, 3, 1090, 581, 57, 2234, 8, 3652, 5917, 78, 33, 19724, 1793, 11, 75711, 479, 42, 497, 127, 4, 15, 975, 7, 695, 8, 16451, 6, 14659, 905, 9, 2394, 10420, 4, 3, 3854, 8, 12997, 2860, 21, 94389, 5, 90765, 5, 6, 41687, 41687, 975, 7, 695, 8, 792, 402, 956, 4, 15, 1152, 11, 7, 59, 825, 8, 16890, 9, 2980, 3, 1432, 6, 1633, 19, 40, 31, 257, 52, 44, 417, 9, 417, 4, 45, 5, 309, 8, 7, 2603, 1473, 75, 3, 4051, 8, 2028, 1268, 520, 1681, 5, 905, 8, 16451, 6, 14659, 5, 6, 212, 8, 402, 956, 11, 279, 93, 36, 75, 5, 2394, 10420, 4]\n",
            "\n",
            "Label: 1\n",
            "Text: I saw this movie on the base movie theater while in the Air Force so my affection for it might be influenced by the reaction of the raucous audience in attendance at the theater that night. But I do think that this movie was one of the first popular kung-fu movies and helped to begin the trend in the early 70's. It's worth seeing.\n",
            "Split: ['i', 'saw', 'this', 'movie', 'on', 'the', 'base', 'movie', 'theater', 'while', 'in', 'the', 'air', 'force', 'so', 'my', 'affection', 'for', 'it', 'might', 'be', 'influenced', 'by', 'the', 'reaction', 'of', 'the', 'raucous', 'audience', 'in', 'attendance', 'at', 'the', 'theater', 'that', 'night', '.', 'but', 'i', 'do', 'think', 'that', 'this', 'movie', 'was', 'one', 'of', 'the', 'first', 'popular', 'kung-fu', 'movies', 'and', 'helped', 'to', 'begin', 'the', 'trend', 'in', 'the', 'early', '70', \"'\", 's', '.', 'it', \"'\", 's', 'worth', 'seeing', '.']\n",
            "Tokens: [14, 216, 15, 20, 28, 3, 2760, 20, 796, 146, 13, 3, 902, 1084, 45, 64, 4118, 22, 12, 237, 35, 3748, 41, 3, 2022, 8, 3, 17720, 311, 13, 13092, 38, 3, 796, 16, 326, 4, 23, 14, 89, 112, 16, 15, 20, 18, 36, 8, 3, 99, 1107, 6328, 107, 6, 1551, 9, 892, 3, 5114, 13, 3, 430, 1417, 10, 17, 4, 12, 10, 17, 271, 316, 4]\n",
            "\n",
            "Number of classes: 2\n",
            "Number of samples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0eLPLiXRWKv",
        "outputId": "cbd8a9c0-4126-44bd-803d-9c4455ba01cb"
      },
      "source": [
        "VALID_DATASET_PATH = '/content/gdrive/MyDrive/Datasets/Text/IMDB_validation_dataset.csv'\n",
        "TEST_DATASET_PATH = '/content/gdrive/MyDrive/Datasets/Text/IMDB_test_with_summary_dataset.csv'\n",
        "\n",
        "valid_dataset = DataFrameDataset(csv_file_path=VALID_DATASET_PATH, only_columns=['label', 'text'])\n",
        "test_dataset = DataFrameDataset(csv_file_path=TEST_DATASET_PATH, only_columns=['label', 'summary'])\n",
        "full_test_dataset = DataFrameDataset(csv_file_path=TEST_DATASET_PATH, only_columns=['label', 'text', 'summary'])\n",
        "print(f'Validation dataset size is: {len(valid_dataset)}')\n",
        "print(f'Test dataset size is: {len(test_dataset)}')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation dataset size is: 16434\n",
            "Test dataset size is: 8467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M3277lZSXXJ",
        "outputId": "80909366-0a25-4726-a936-bd427cc2d3e0"
      },
      "source": [
        "test_loader = DataLoader(full_test_dataset, batch_size=2, shuffle=False)\n",
        "for batch in test_loader:\n",
        "  print('Print samples from a single batch:\\n')\n",
        "  labels, texts, summaries = batch\n",
        "  for i in range(len(labels)):\n",
        "    print(f'Sample: {i}')\n",
        "    print(f'Label: {labels[i]}')\n",
        "    print(f'Text: {texts[i]}')\n",
        "    print(f'Label: {summaries[i]}')\n",
        "    print('\\n')\n",
        "  break"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Print samples from a single batch:\n",
            "\n",
            "Sample: 0\n",
            "Label: neg\n",
            "Text: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to maki\n",
            "Label: I AM CURIOUS-YELLOW is a film about a young Swedish drama student named Lena who wants to learn everything she can about life. The plot is centered around a maki girl named Maki who wants to focus her attentions on the maki boy's maki, which is a game of maki. The film was released in 1967 and has been rated 4/5 (Sweden).\n",
            "\n",
            "\n",
            "Sample: 1\n",
            "Label: neg\n",
            "Text: \"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swing\n",
            "Label: \"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. Where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFw4dSbJWODO"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)    "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivnGdjNMWOFz"
      },
      "source": [
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        x = self.fc(embedded)\n",
        "        output = self.activation(x)\n",
        "        return output"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7Ms8i1-WOIJ"
      },
      "source": [
        "train_iter = IMDB(split='test')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR7TagKqWOK1"
      },
      "source": [
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 100\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text, offsets)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predited_label = model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IkN1G6mWONN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "outputId": "e230efba-cfbf-4b7d-a31d-b8877eea669a"
      },
      "source": [
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 0.001  # learning rate\n",
        "BATCH_SIZE = 32 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter = IMDB(split='test')\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "# test_dataset = to_map_style_dataset(test_iter)\n",
        "# num_train = int(len(train_dataset) * 0.95)\n",
        "# split_train_, split_valid_ = \\\n",
        "#     random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    # if total_accu is not None and total_accu > accu_val:\n",
        "    #   scheduler.step()\n",
        "    # else:\n",
        "    total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   100/  782 batches | accuracy    0.493\n",
            "| epoch   1 |   200/  782 batches | accuracy    0.498\n",
            "| epoch   1 |   300/  782 batches | accuracy    0.497\n",
            "| epoch   1 |   400/  782 batches | accuracy    0.498\n",
            "| epoch   1 |   500/  782 batches | accuracy    0.500\n",
            "| epoch   1 |   600/  782 batches | accuracy    0.500\n",
            "| epoch   1 |   700/  782 batches | accuracy    0.509\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-ea12da9b28c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0maccu_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m# if total_accu is not None and total_accu > accu_val:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#   scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-a7d417cfdda0>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mpredited_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredited_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-8bc4a2ce8ade>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monly_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mrow_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrow_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0;31m# handle the dup indexing case GH#4246\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_values_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1097\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0;34m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m         )\n\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[1;32m   4530\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4531\u001b[0m                 \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_dups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4532\u001b[0;31m                 \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4533\u001b[0m             )\n\u001b[1;32m   4534\u001b[0m             \u001b[0;31m# If we've made a copy once, no need to make another one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m             \u001b[0mnew_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice_take_blocks_ax0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             new_blocks = [\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_value, only_slice)\u001b[0m\n\u001b[1;32m   1360\u001b[0m                             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m                             \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msllen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m                             \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m                         )\n\u001b[1;32m   1364\u001b[0m                     ]\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         new_values = algos.take_nd(\n\u001b[0;32m-> 1256\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m         )\n\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     func = _get_take_nd_function(\n\u001b[0;32m-> 1735\u001b[0;31m         \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1736\u001b[0m     )\n\u001b[1;32m   1737\u001b[0m     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36m_get_take_nd_function\u001b[0;34m(ndim, arr_dtype, out_dtype, axis, mask_info)\u001b[0m\n\u001b[1;32m   1513\u001b[0m ):\n\u001b[1;32m   1514\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marr_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_take_1d_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_name_get\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;31m# append bit counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_name_includes_bit_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_name_includes_bit_suffix\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_name_includes_bit_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;31m# pointer size varies by system, best to omit it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4wj_dhQWOPL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc988797-d76a-4d24-a648-65eea1b20b9b"
      },
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZiqQEhbWjti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17727272-cb54-4d0b-94f0-c60dc354105c"
      },
      "source": [
        "ag_news_label = {1: \"World\",\n",
        "                 2: \"Sports\",\n",
        "                 3: \"Business\",\n",
        "                 4: \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, text_pipeline):\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor(text_pipeline(text))\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a Sports news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7DN6NobWjwJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}