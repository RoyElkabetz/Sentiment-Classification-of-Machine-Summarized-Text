{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Summarization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPuGqv7zfLJJLuHTcNrCHTJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2148bb51fd1b407a9f358dbbffe019f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3a26e6d8c5ff4bb7b066d9e1b714c4f7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1ab81ae778d346fbad973d2b0b014434",
              "IPY_MODEL_b31f239b8ca84fa3ad4662454908e397"
            ]
          }
        },
        "3a26e6d8c5ff4bb7b066d9e1b714c4f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ab81ae778d346fbad973d2b0b014434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3c89c411b3c04d1da181edde6e68c29b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b4124fac8e234d3ebfbed0ffc9cad63c"
          }
        },
        "b31f239b8ca84fa3ad4662454908e397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3c3ed35ec87b40d39ef20912e3c7e10a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10/10 [36:45&lt;00:00, 220.57s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f9acee365c0940918425f506a21eb6c9"
          }
        },
        "3c89c411b3c04d1da181edde6e68c29b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b4124fac8e234d3ebfbed0ffc9cad63c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c3ed35ec87b40d39ef20912e3c7e10a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f9acee365c0940918425f506a21eb6c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6afaa31c3a974275923c653865c528ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_83b8dd1eb6fc4dfd84e9b0f80587897a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a63ab65c4d344e038c456a3a35a12c23",
              "IPY_MODEL_0bd8eb7956bd4233939788d0c0a2cb39"
            ]
          }
        },
        "83b8dd1eb6fc4dfd84e9b0f80587897a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a63ab65c4d344e038c456a3a35a12c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_21ff8993bad34161a8a3f34a6bbe7f1d",
            "_dom_classes": [],
            "description": "  3%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 97,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15b526fe3097482dbd3ae76f01610d31"
          }
        },
        "0bd8eb7956bd4233939788d0c0a2cb39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9d214df0bd57460a836254092067a87f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/97 [00:00&lt;00:11,  8.10it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f7745ed739cc46129fde039120a58a01"
          }
        },
        "21ff8993bad34161a8a3f34a6bbe7f1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15b526fe3097482dbd3ae76f01610d31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d214df0bd57460a836254092067a87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f7745ed739cc46129fde039120a58a01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoyElkabetz/Text-Summarization-with-Deep-Learning/blob/main/Text_Summarization_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94-50bV31g5A",
        "outputId": "825d6123-2eec-4a8e-be8a-3a7c7be45edf"
      },
      "source": [
        "## uncomment only if running from google.colab\n",
        "# clone the git reposetory\n",
        "!git clone https://github.com/RoyElkabetz/Text-Summarization-with-Deep-Learning\n",
        "# add path to .py files for import\n",
        "import sys\n",
        "sys.path.insert(1, \"/content/Text-Summarization-with-Deep-Learning\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Text-Summarization-with-Deep-Learning'...\n",
            "remote: Enumerating objects: 107, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 107 (delta 61), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (107/107), 106.62 KiB | 2.01 MiB/s, done.\n",
            "Resolving deltas: 100% (61/61), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ejtaOc71hzT",
        "outputId": "6f9f6451-705f-44ac-9dba-37220b122ccb"
      },
      "source": [
        "## uncomment if you want to mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CniCusU62lza"
      },
      "source": [
        "PATH_DATASET = '/content/gdrive/MyDrive/Datasets/Text/news_summary_more.csv'\n",
        "CHECKPOINT_DIR = '/content/gdrive/MyDrive/Checkpoints'\n",
        "MODEL_NAME = 'LSTM_Text_Summarizer'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZxlt_woxho8",
        "outputId": "55d45e8b-7de0-4913-b8c7-17fa563e8308"
      },
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, display, clear_output\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchtext.legacy.data as data\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.legacy.data import Field, Dataset, Example, BucketIterator, Iterator, TabularDataset\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# from torchtext.datasets import IMDB as the_dataset\n",
        "# from torchtext.datasets import AG_NEWS as the_dataset\n",
        "# import torchtext.data as data\n",
        "# from torchtext.data.utils import get_tokenizer\n",
        "# from torchtext.vocab import build_vocab_from_iterator\n",
        "# from torchtext.data.functional import to_map_style_dataset\n",
        "# from torch.utils.data import DataLoader\n",
        "# from torch.utils.data.dataset import random_split\n",
        "# from torch import nn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f'torch {torch.__version__}')\n",
        "print('Device properties:')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    gpu_data = torch.cuda.get_device_properties(0)\n",
        "    gpu_name = gpu_data.name\n",
        "    gpu_mem  = f'{gpu_data.total_memory * 1e-9:.02f} Gb'\n",
        "    print(f'GPU: {gpu_name}\\nMemory: {gpu_mem}')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('CPU')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch 1.9.0+cu102\n",
            "Device properties:\n",
            "CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p6Ut6L0t4fQ"
      },
      "source": [
        "# class DataFrameDataset(Dataset):\n",
        "#     \"\"\"Class for using pandas DataFrames as a datasource\"\"\"\n",
        "#     \"\"\"Pytorch legacy Dataset: https://pytorch.org/text/_modules/torchtext/data/dataset.html\"\"\"\n",
        "#     def __init__(self, examples, fields, filter_pred=None):\n",
        "#         \"\"\"\n",
        "#         Create a dataset from a pandas dataframe of examples and Fields\n",
        "#         Arguments:\n",
        "#             examples pd.DataFrame: DataFrame of examples\n",
        "#             fields {str: Field}: The Fields to use in this tuple. The\n",
        "#                 string is a field name, and the Field is the associated field.\n",
        "#             filter_pred (callable or None): use only exanples for which\n",
        "#                 filter_pred(example) is true, or use all examples if None.\n",
        "#                 Default is None\n",
        "#         \"\"\"\n",
        "#         self.examples = examples.apply(SeriesExample.fromSeries, args=(fields,), axis=1).tolist()\n",
        "#         if filter_pred is not None:\n",
        "#             self.examples = filter(filter_pred, self.examples)\n",
        "#         self.fields = dict(fields)\n",
        "#         # Unpack field tuples\n",
        "#         for n, f in list(self.fields.items()):\n",
        "#             if isinstance(n, tuple):\n",
        "#                 self.fields.update(zip(n, f))\n",
        "#                 del self.fields[n]\n",
        "        \n",
        "\n",
        "#         # if filter_pred is not None:\n",
        "#         #     make_list = isinstance(examples, list)\n",
        "#         #     examples = filter(filter_pred, examples)\n",
        "#         #     if make_list:\n",
        "#         #         examples = list(examples)\n",
        "#         # self.examples = examples\n",
        "#         # self.fields = dict(fields)\n",
        "#         # # Unpack field tuples\n",
        "        \n",
        "#         for n, f in list(self.fields.items()):\n",
        "#             if isinstance(n, tuple):\n",
        "#                 self.fields.update(zip(n, f))\n",
        "#                 del self.fields[n]\n",
        "#         print('you are here')\n",
        "    \n",
        "#     def __len__(self):\n",
        "#         # return length of examples\n",
        "#         try:\n",
        "#             return len(self.examples)\n",
        "#         except TypeError:\n",
        "#             return 2**32\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         # get items from examples\n",
        "#         return self.examples[index]\n",
        "    \n",
        "    \n",
        "#     def __iter__(self):\n",
        "#         # iterator for batch processing\n",
        "#         for x in self.examples:\n",
        "#             yield x\n",
        "\n",
        "            \n",
        "#     def __getattr__(self, attr):\n",
        "#         if attr in self.fields:\n",
        "#             for x in self.examples:\n",
        "#                 yield getattr(x, attr)\n",
        "\n",
        "# class SeriesExample(Example):\n",
        "#     \"\"\"Class to convert a pandas Series to an Example\"\"\"\n",
        "  \n",
        "#     classmethod\n",
        "#     def fromSeries(cls, data, fields):\n",
        "#         return cls.fromdict(data.to_dict(), fields)\n",
        "\n",
        "#     classmethod\n",
        "#     def fromdict(cls, data, fields):\n",
        "#         ex = cls()\n",
        "        \n",
        "#         for key, field in fields.items():\n",
        "#             if key not in data:\n",
        "#                 raise ValueError(\"Specified key {} was not found in \"\n",
        "#                 \"the input data\".format(key))\n",
        "#             if field is not None:\n",
        "#                 setattr(ex, key, field.preprocess(data[key]))\n",
        "#             else:\n",
        "#                 setattr(ex, key, data[key])\n",
        "#         return ex"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PoxFpA7H1ly"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06qoDRox3j81",
        "outputId": "d7656840-175d-4d4d-d0b0-035682bc64d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# define the text field for the dataset\n",
        "TEXT = data.Field(sequential=True,\n",
        "                  lower=True,\n",
        "                  tokenize=tokenizer,\n",
        "                  init_token='<sos>', \n",
        "                  eos_token='<eos>',\n",
        "                  dtype=torch.long)\n",
        "# define the text field for the dataset\n",
        "HEADLINES = data.Field(sequential=True,\n",
        "                    lower=True, \n",
        "                    tokenize=tokenizer,\n",
        "                    init_token='<sos>', \n",
        "                    eos_token='<eos>',\n",
        "                    dtype=torch.long)\n",
        "\n",
        "fields = {'headlines': ('headlines', HEADLINES), 'text': ('text', TEXT)}\n",
        "the_dataset = TabularDataset(PATH_DATASET, 'CSV', fields)\n",
        "data_iterator = Iterator(the_dataset, batch_size=20, device=device)\n",
        "# TEXT.build_vocab(the_dataset)\n",
        "# HEADLINES.build_vocab(the_dataset)\n",
        "# SUMMARY.build_vocab(the_dataset) "
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-80bad97ceae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'headlines'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'headlines'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHEADLINES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mthe_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabularDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_DATASET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CSV'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# TEXT.build_vocab(the_dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'fields'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVY_o_muDYGi",
        "outputId": "910ffaf0-e98d-480a-c1a1-cdd189478aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "vocab = build_vocab_from_iterator(map(tokenizer, data_iterator), specials=[\"<unk>\"])"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-ddc406b35bf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, min_freq, specials, special_first)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \"\"\"\n\u001b[1;32m    230\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Field' object has no attribute 'vocab'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVUpHJ0PBfCj"
      },
      "source": [
        "def data_process(raw_text_iter):\n",
        "  data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIYG4hX-Bj6o",
        "outputId": "9a24ad62-0335-4ae5-ab6b-d3e4d6853e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "train_data = data_process(data_iterator)\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-7f7a2da907c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-85-ed22a4227f6a>\u001b[0m in \u001b[0;36mdata_process\u001b[0;34m(raw_text_iter)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_text_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-ed22a4227f6a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_text_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/data/utils.py\u001b[0m in \u001b[0;36m_basic_english_normalize\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpattern_re\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplaced_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_patterns_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplaced_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhiUzZdAATbX",
        "outputId": "7b606b3d-7d43-4adb-b28e-e5069dabf30e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "source": [
        "vocab = build_vocab_from_iterator(map(tokenizer, data_iterator), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-d9ab19b59fd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, min_freq, specials, special_first)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \"\"\"\n\u001b[1;32m    230\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Field' object has no attribute 'vocab'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMjAID2J4h9r",
        "outputId": "775bdc4c-9128-4620-f75d-d8791ab13007",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(the_dataset[0].text)\n",
        "print(the_dataset[0].headlines)\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['saurav', 'kant', ',', 'an', 'alumnus', 'of', 'upgrad', 'and', 'iiit-b', \"'\", 's', 'pg', 'program', 'in', 'machine', 'learning', 'and', 'artificial', 'intelligence', ',', 'was', 'a', 'sr', 'systems', 'engineer', 'at', 'infosys', 'with', 'almost', '5', 'years', 'of', 'work', 'experience', '.', 'the', 'program', 'and', 'upgrad', \"'\", 's', '360-degree', 'career', 'support', 'helped', 'him', 'transition', 'to', 'a', 'data', 'scientist', 'at', 'tech', 'mahindra', 'with', '90%', 'salary', 'hike', '.', 'upgrad', \"'\", 's', 'online', 'power', 'learning', 'has', 'powered', '3', 'lakh+', 'careers', '.']\n",
            "['upgrad', 'learner', 'switches', 'to', 'career', 'in', 'ml', '&', 'al', 'with', '90%', 'salary', 'hike']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CddpG0O-4iAG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3idaIvG4iCs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGHhDHWuw5a8"
      },
      "source": [
        "# class DataFrame_Dataset(Dataset):\n",
        "\n",
        "#     def __init__(self, path, fields, **kwargs):\n",
        "        \n",
        "#         # path of directory containing inputs\n",
        "#         self.path = path\n",
        "#         # initialize fileds\n",
        "#         if not isinstance(fields[0], (tuple, list)):\n",
        "#             fields = [('text', fields[0]), ('summary', fields[1])]\n",
        "        \n",
        "#         # read Articles and summaries into pandas dataframe\n",
        "#         self.news_list = self._read_data()\n",
        "#         # load articles as torch text examples\n",
        "#         # I am not doing text pre-processing although I have written code for that\n",
        "#         examples = [Example.fromlist(list(item), fields) for item in self.news_list] \n",
        "#         # initialize\n",
        "#         super().__init__(examples, fields, **kwargs)\n",
        "        \n",
        "\n",
        "#     def __len__(self):\n",
        "#         # return length of examples\n",
        "#         try:\n",
        "#             return len(self.examples)\n",
        "#         except TypeError:\n",
        "#             return 2**32\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         # get items from examples\n",
        "#         return self.examples[index]\n",
        "    \n",
        "    \n",
        "#     def __iter__(self):\n",
        "#         # iterator for batch processing\n",
        "#         for x in self.examples:\n",
        "#             yield x\n",
        "\n",
        "            \n",
        "#     def __getattr__(self, attr):\n",
        "#         if attr in self.fields:\n",
        "#             for x in self.examples:\n",
        "#                 yield getattr(x, attr)\n",
        "    \n",
        "    \n",
        "#     # function to read text files into pandas data frame\n",
        "#     def _read_data(self):\n",
        "#         # initialize variables\n",
        "#         Articles=[]\n",
        "#         Summaries=[]\n",
        "        \n",
        "#         # loop over all files and read them into lists\n",
        "#         for d,path,filenames in tqdm(os.walk(self.path)):\n",
        "#             for file in filenames:\n",
        "#                 if os.path.isfile(d+'/'+file):\n",
        "#                     if('Summaries' in d+'/'+file):\n",
        "#                         with open(d+'/'+file,'r',errors='ignore') as f:\n",
        "#                             summary=' '.join([i.rstrip() for i in f.readlines()])\n",
        "#                             Summaries.append(summary)\n",
        "#                     else:\n",
        "#                         with open(d+'/'+file,'r',errors='ignore') as f:\n",
        "#                             Article=' '.join([i.rstrip() for i in f.readlines()])\n",
        "#                             Articles.append(Article)\n",
        "        \n",
        "#         return zip(Articles, Summaries)\n",
        "    \n",
        "\n",
        "#     # functions for pre-processing data\n",
        "#     # clean text data\n",
        "#     def _clean_data(self, text):\n",
        "#         # remove links\n",
        "#         text = self._remove_links(text)\n",
        "#         # remove numbers\n",
        "#         text = self._remove_numbers(text)\n",
        "#         # remove punctuations\n",
        "#         text = self._remove_punct(text)\n",
        "#         # word_list = self.tokenizer(text)\n",
        "#         # word_list = self._get_root(word_list)\n",
        "\n",
        "#         return text.lower()\n",
        "    \n",
        "#     # remove punctuations\n",
        "#     def _remove_punct(self, text):\n",
        "#         nopunct = ''\n",
        "#         for c in text:\n",
        "#             if c not in string.punctuation:\n",
        "#                 nopunct = nopunct + c\n",
        "#         return nopunct\n",
        "\n",
        "#     # remove numbers\n",
        "#     def _remove_numbers(self, text):\n",
        "#         return re.sub(r'[0-9]', '', text)\n",
        "\n",
        "#     # remove links\n",
        "#     def _remove_links(self, text):\n",
        "#         return re.sub(r'http\\S+', '', text)\n",
        "    \n",
        "#     # stemming\n",
        "#     def _get_root(self, word_list):\n",
        "#         ps = PorterStemmer()\n",
        "#         return [ps.stem(word) for word in word_list]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "DgDn9Xxy3FqN",
        "outputId": "8794e516-754b-49e2-d7d0-097f6b2e5c39"
      },
      "source": [
        "data_df = pd.read_csv(PATH_DATASET ,encoding='utf-8')\n",
        "data_df.drop_duplicates(subset=['text'],inplace=True)  # dropping duplicates\n",
        "data_df.dropna(axis=0,inplace=True)  # dropping na\n",
        "data_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
              "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
              "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
              "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
              "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
              "      <td>Speaking about the sexual harassment allegatio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           headlines                                               text\n",
              "0  upGrad learner switches to career in ML & Al w...  Saurav Kant, an alumnus of upGrad and IIIT-B's...\n",
              "1  Delhi techie wins free food from Swiggy for on...  Kunal Shah's credit card bill payment platform...\n",
              "2  New Zealand end Rohit Sharma-led India's 12-ma...  New Zealand defeated India by 8 wickets in the...\n",
              "3  Aegon life iTerm insurance plan helps customer...  With Aegon Life iTerm Insurance plan, customer...\n",
              "4  Have known Hirani for yrs, what if MeToo claim...  Speaking about the sexual harassment allegatio..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDhBVBMb3dp8"
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiAmXOc0378W",
        "outputId": "66f524d1-323e-4a7d-f704-e97165997866"
      },
      "source": [
        "## run this if you want to remove stop words from data\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower() # lowercase\n",
        "    text = text.split() # convert have'nt -> have not\n",
        "    for i in range(len(text)):\n",
        "        word = text[i]\n",
        "        if word in contraction_mapping:\n",
        "            text[i] = contraction_mapping[word]\n",
        "    text = \" \".join(text)\n",
        "    text = text.split()\n",
        "    newtext = []\n",
        "    for word in text:\n",
        "        if word not in stop_words:\n",
        "            newtext.append(word)\n",
        "    text = \" \".join(newtext)\n",
        "    text = text.replace(\"'s\",'') # convert your's -> your\n",
        "    text = re.sub(r'\\(.*\\)','',text) # remove (words)\n",
        "    text = re.sub(r'[^a-zA-Z0-9. ]','',text) # remove punctuations\n",
        "    text = re.sub(r'\\.',' . ',text)\n",
        "    return text\n",
        "\n",
        "sample = \"(hello) hi there .man tiger caller who's that isn't it ? WALL-E\"\n",
        "print(preprocess(sample))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            " hi  . man tiger caller  walle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9I6cfkx5ED3",
        "outputId": "61985b80-bd39-4c6b-ae1e-c5391e071510"
      },
      "source": [
        "# process the data inplace\n",
        "print('Before preprocessing:\\n {}\\n {}\\n'.format(data_df['headlines'][20], data_df['text'][20]))\n",
        "data_df['cleaned_headlines'] = data_df['headlines'].apply(lambda x: preprocess(x))\n",
        "data_df['cleaned_text'] = data_df['text'].apply(lambda x: preprocess(x))\n",
        "print('After preprocessing:\\n {}\\n {}\\n'.format(data_df['headlines'][20], data_df['text'][20]))\n",
        "print('After preprocessing:\\n {}\\n {}\\n'.format(data_df['cleaned_headlines'][20], data_df['cleaned_text'][20]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before preprocessing:\n",
            " I think the opposition even dreams about me: PM Modi\n",
            " Claiming there is a dearth of ideas among opposition parties, Prime Minister Narendra Modi on Wednesday said, \"The opposition talks only about Modi the whole day, I suspect they even dream about me.\" PM Modi, who was addressing the New India Youth Conclave inâ Surat, added that the opposition parties have only one agenda which is \"Modi\". \n",
            "\n",
            "After preprocessing:\n",
            " I think the opposition even dreams about me: PM Modi\n",
            " Claiming there is a dearth of ideas among opposition parties, Prime Minister Narendra Modi on Wednesday said, \"The opposition talks only about Modi the whole day, I suspect they even dream about me.\" PM Modi, who was addressing the New India Youth Conclave inâ Surat, added that the opposition parties have only one agenda which is \"Modi\". \n",
            "\n",
            "After preprocessing:\n",
            " think opposition even dreams me pm modi\n",
            " claiming dearth ideas among opposition parties prime minister narendra modi wednesday said the opposition talks modi whole day suspect even dream me .  pm modi addressing new india youth conclave in surat added opposition parties one agenda modi . \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceSbOcqp6m-c"
      },
      "source": [
        "data_df.replace('', np.nan, inplace=True)\n",
        "data_df.dropna(axis=0, inplace=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ndNK3O3J9tTZ",
        "outputId": "90dca5ae-d206-408f-a0f7-a6c0bdd8cce9"
      },
      "source": [
        "text_word_count = []\n",
        "headlines_word_count = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in data_df['cleaned_text']:\n",
        "      text_word_count.append(len(i.split()))\n",
        "\n",
        "for i in data_df['cleaned_headlines']:\n",
        "      headlines_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'cleaned_text':text_word_count, 'cleaned_headlines':headlines_word_count})\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsdZeXQC-Gw9",
        "outputId": "6292e0aa-514b-4d76-eb54-cf49c350d298"
      },
      "source": [
        "cnt = 0\n",
        "for i in data_df['cleaned_headlines']:\n",
        "    if(len(i.split()) <= 10):\n",
        "        cnt += 1\n",
        "print(cnt / len(data_df['cleaned_headlines']))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9874644164294428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SZHjWgw-NAm"
      },
      "source": [
        "max_text_len = 50\n",
        "max_summary_len = 10"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2IMDGBa--mH"
      },
      "source": [
        "cleaned_text = np.array(data_df['cleaned_text'])\n",
        "cleaned_summary = np.array(data_df['cleaned_headlines'])\n",
        "pad_token = ' <pad>'\n",
        "\n",
        "\n",
        "short_text = []\n",
        "short_summary = []\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    if(len(cleaned_summary[i].split()) <= max_summary_len and len(cleaned_text[i].split()) <= max_text_len):\n",
        "        padded_cleaned_text = cleaned_text[i] + (max_text_len - len(cleaned_text[i].split())) * pad_token\n",
        "        padded_cleaned_summary = cleaned_summary[i] + (max_summary_len - len(cleaned_summary[i].split())) * pad_token\n",
        "        padded_cleaned_summary = '<sos> ' + padded_cleaned_summary + ' <eos>'\n",
        "        short_text.append(padded_cleaned_text)\n",
        "        short_summary.append(padded_cleaned_summary)\n",
        "        \n",
        "df = pd.DataFrame({'text': short_text,'summary': short_summary})"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnJw_BX-AEBv"
      },
      "source": [
        "# df['summary'] = df['summary'].apply(lambda x: '<sos> '+ x + ' <eos>')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2Po4j1tATeO"
      },
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# define the text field for the dataset\n",
        "TEXT = data.Field(sequential=True,\n",
        "                  lower=True, \n",
        "                  tokenize=tokenizer,\n",
        "                  init_token='<sos>', \n",
        "                  eos_token='<eos>',\n",
        "                  dtype=torch.long)\n",
        "# define the text field for the dataset\n",
        "SUMMARY = data.Field(sequential=True,\n",
        "                    lower=True, \n",
        "                    tokenize=tokenizer,\n",
        "                    init_token='<sos>', \n",
        "                    eos_token='<eos>',\n",
        "                    dtype=torch.long)\n",
        "#SUMMARY = data.Field(sequential=True, tokenize=tokenizer)\n",
        "# TEXT.build_vocab(my_data, max_size=25000, vectors=\"glove.6B.100d\") \n",
        "#TEXT.build_vocab(my_data, max_size=25000) \n",
        "#SUMMARY.build_vocab(my_data)\n",
        "fields = {'text': TEXT, 'summary': SUMMARY}"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yv6xCz8Btxez",
        "outputId": "e23d06b6-169d-45e9-8ba0-6175452e23bf"
      },
      "source": [
        "tttt = DataFrameDataset(df, fields=fields)\n",
        "# train_data, test_data, val_data = DataFrameDataset(df, fields=fields).split(split_ratio=[0.8, 0.1, 0.1])\n",
        "a, b, c = tttt.split(split_ratio=[0.8, 0.1, 0.1])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you are here\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSEhPx0z5oZc",
        "outputId": "d7122fee-2d9b-44b1-cb55-65c4e4f5a8a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(len(the_dataset)):\n",
        "    if i<10:\n",
        "        print(the_dataset.examples[i])\n",
        "    if len(the_dataset[i].text)==0 or len(the_dataset[i].headlines)==0:\n",
        "        print(i)\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torchtext.legacy.data.example.Example object at 0x7f8d7a948f90>\n",
            "<torchtext.legacy.data.example.Example object at 0x7f8d7a948f50>\n",
            "<torchtext.legacy.data.example.Example object at 0x7f8d7a948cd0>\n",
            "<torchtext.legacy.data.example.Example object at 0x7f8d7a948c90>\n",
            "<torchtext.legacy.data.example.Example object at 0x7f8d7a8ce950>\n",
            "<torchtext.legacy.data.example.Example object at 0x7f8d7a8ce510>\n",
            "<torchtext.legacy.data.example.Example object at 0x7f8d7a8fd150>\n",
            "<torchtext.legacy.data.example.Example object at 0x7f8d7a8dd950>\n",
            "<torchtext.legacy.data.example.Example object at 0x7f8d7a8ce0d0>\n",
            "<torchtext.legacy.data.example.Example object at 0x7f8d7a8fd190>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NwkFMtj2DDg",
        "outputId": "b56ce9c1-069c-446c-91bf-07e30bd9813e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "vocab = build_vocab_from_iterator(map(tokenizer, the_dataset), specials=[\"<unk>\"])\n",
        "# vocab = TEXT.build_vocab(the_dataset)\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0lines [00:00, ?lines/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-e74220b12a14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# vocab = TEXT.build_vocab(the_dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/vocab.py\u001b[0m in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, num_lines)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lines'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_lines\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m             \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/data/utils.py\u001b[0m in \u001b[0;36m_basic_english_normalize\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpattern_re\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplaced_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_patterns_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplaced_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Example' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_l8aWWP0LTr"
      },
      "source": [
        "TEXT.build_vocab(train_data)\n",
        "SUMMARY.build_vocab(train_data) \n",
        "\n",
        "vocab = TEXT.vocab"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fXNzViun-Ml",
        "outputId": "011e0575-d3bb-46a7-e434-cbfc18959b99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "a = the_dataset\n",
        "print(a)\n",
        "def data_process(raw_text_iter):\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    #return data\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "b = data_process(a)\n",
        "print(b)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torchtext.legacy.data.dataset.TabularDataset object at 0x7f8dd39a1f10>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-8836e39be4df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#return data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-8836e39be4df>\u001b[0m in \u001b[0;36mdata_process\u001b[0;34m(raw_text_iter)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_text_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#return data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-8836e39be4df>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_text_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#return data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/data/utils.py\u001b[0m in \u001b[0;36m_basic_english_normalize\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpattern_re\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplaced_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_patterns_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplaced_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Example' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "rVqBCyuyvxry",
        "outputId": "2d0202e2-af66-45a0-f867-a1bc9e130300"
      },
      "source": [
        "print(f'Number of train samples: {len(train_data)}')\n",
        "print(f'Number of validation samples: {len(val_data)}')\n",
        "print(f'Number of test samples: {len(test_data)}')\n",
        "\n",
        "# display data samples\n",
        "display(HTML('<h4>Display data samples:</h4>'))\n",
        "n_samples = 2\n",
        "for i in range(n_samples):\n",
        "    print(\"\\nText:\\n \", \" \".join([t for t in train_data.examples[i].text]))\n",
        "    print(\"Tokens:\\n \", [vocab.stoi[t] for t in train_data.examples[i].text])\n",
        "    print(\"Summary:\\n \", \" \".join([t for t in train_data.examples[i].summary]))\n",
        "    print(\"Tokens:\\n \", [vocab.stoi[t] for t in train_data.examples[i].summary])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of train samples: 76906\n",
            "Number of validation samples: 9613\n",
            "Number of test samples: 9613\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h4>Display data samples:</h4>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Text:\n",
            "  kangana ranaut said never privilege dumping anyone adding i always got dumped . . . they come back cannot take back moved another loser . i many affairs every breakup feel like end love life said . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Tokens:\n",
            "  [1019, 1798, 5, 301, 7129, 7697, 956, 126, 33, 604, 200, 6067, 4, 4, 4, 1095, 280, 194, 242, 108, 194, 1162, 164, 18132, 4, 33, 208, 613, 267, 8998, 842, 39, 410, 418, 163, 5, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Summary:\n",
            "  <sos> never privilege dumping someone kangana ranaut <pad> <pad> <pad> <pad> <eos>\n",
            "Tokens:\n",
            "  [2, 301, 7129, 7697, 898, 1019, 1798, 1, 1, 1, 1, 3]\n",
            "\n",
            "Text:\n",
            "  actor vivek oberoi revealed lived slums used public toilets get role 2002 film company director ram gopal varma found goodlooking part . added when finally met him i . . . kept legs desk said ey yeh thobda kya dekh raha hai . convinced him . <pad> <pad> <pad> <pad>\n",
            "Tokens:\n",
            "  [55, 4817, 8234, 87, 3856, 13484, 98, 184, 2783, 123, 357, 2578, 16, 35, 220, 353, 4357, 5749, 82, 20047, 117, 4, 6, 985, 4107, 608, 177, 33, 4, 4, 4, 1158, 3598, 9021, 5, 30167, 6602, 85459, 5212, 19872, 12017, 1050, 4, 6916, 177, 4, 1, 1, 1, 1]\n",
            "Summary:\n",
            "  <sos> lived slum used public toilet get company role vivek <pad> <eos>\n",
            "Tokens:\n",
            "  [2, 3856, 9458, 98, 184, 1721, 123, 35, 357, 4817, 1, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtTxuiLxwJ6J",
        "outputId": "d6974447-98a3-4997-e424-2e343f35503f"
      },
      "source": [
        "print(vars(train_data.examples[-1]))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['trailer', 'upcoming', 'zareen', 'khan', 'karan', 'kundrra', 'starrer', 'horror', 'film', '1921', 'released', '.', 'fourth', 'film', '1920', 'franchise', 'revolves', 'around', 'lead', 'characters', 'facing', 'extreme', 'paranormal', 'activities', 'created', 'cursed', 'spirit', '.', 'directed', 'vikram', 'bhatt', 'film', 'scheduled', 'release', 'january', '12', '2018', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], 'summary': ['<sos>', 'trailer', 'zareen', 'karan', 'kundrra', 'horror', 'film', '1921', 'released', '<pad>', '<pad>', '<eos>']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjzvUwk_xnft"
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = Iterator.splits(\n",
        "    (train_data, val_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device, \n",
        "    sort_key=lambda x: len(x.text))"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZmFkO-q5iKB"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        # initializations\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        # we will use 2 layers for both encoder and decoder\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        return hidden, cell\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        # initialize\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        # for decoder we will use n_directions 1\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        # fully connected layer to predict words\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, hidden, cell):\n",
        "        \n",
        "        #trg = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n directions in the decoder will always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "        \n",
        "        trg = trg.unsqueeze(0)\n",
        "        \n",
        "        #trg = [1, batch size]\n",
        "        embedded = self.dropout(self.embedding(trg))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden, cell\n",
        "\n",
        "    \n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \"Encoder and decoder must have equal number of layers!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size] where src_len is number of tokens in source sentence\n",
        "        #trg = [trg len, batch size] same for trg_len\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim # we don't have trg.shape[-1] here\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        dec_input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(dec_input, hidden, cell)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            dec_input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O3xrMfU6kPC",
        "outputId": "10bae506-6f6a-4652-ead8-397628467d00"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "OUTPUT_DIM = len(SUMMARY.vocab)\n",
        "ENC_EMB_DIM = 32\n",
        "DEC_EMB_DIM = 32\n",
        "HID_DIM = 64\n",
        "N_LAYERS = 1\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "# initialize seq2seq model\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "model = Seq2Seq(enc, dec, device)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMJo4oMo7Udu",
        "outputId": "41de53b8-1529-4a93-c6d6-cbb3d20e7b56"
      },
      "source": [
        "vocab.stoi['<pad>']"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBp1ZBSl61DE"
      },
      "source": [
        "class Seq2Seq_trainer(object):\n",
        "    def __init__(self, model, train_iterator, valid_iterator, pad_index, device, clip, learning_rate):\n",
        "        # initialize config variables\n",
        "        self.model = model.to(device)\n",
        "        self.train_iterator = train_iterator\n",
        "        self.valid_iterator = valid_iterator\n",
        "        self.clip = clip\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        # TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index = pad_index)\n",
        "        self.model.apply(self.init_weights)\n",
        "        print(f'The model has {self.count_parameters(self.model):,} trainable parameters')\n",
        "\n",
        "        \n",
        "    \n",
        "    def init_weights(self,m):\n",
        "        for name, param in m.named_parameters():\n",
        "            nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "    \n",
        "    def count_parameters(self, model):\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    \n",
        "    def train(self):\n",
        "\n",
        "        self.model.train()\n",
        "        epoch_loss = 0\n",
        "        for i, batch in enumerate(self.train_iterator):\n",
        "\n",
        "            src = batch.text\n",
        "            trg = batch.summary\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(src, trg)\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = self.criterion(output, trg)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
        "            self.optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        return epoch_loss / len(self.train_iterator)\n",
        "    \n",
        "    \n",
        "    def evaluate(self, iterator):\n",
        "\n",
        "        self.model.eval()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(iterator):\n",
        "\n",
        "                src = batch.text\n",
        "                trg = batch.summary\n",
        "                output = self.model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "                #trg = [trg len, batch size]\n",
        "                #output = [trg len, batch size, output dim]\n",
        "\n",
        "                output_dim = output.shape[-1]\n",
        "                output = output[1:].view(-1, output_dim)\n",
        "                trg = trg[1:].view(-1)\n",
        "\n",
        "                #trg = [(trg len - 1) * batch size]\n",
        "                #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "                loss = self.criterion(output, trg)\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "        return epoch_loss / len(iterator)\n",
        "    \n",
        "    \n",
        "    def epoch_time(self, start_time, end_time):\n",
        "        elapsed_time = end_time - start_time\n",
        "        elapsed_mins = int(elapsed_time / 60)\n",
        "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "        return elapsed_mins, elapsed_secs\n",
        "    \n",
        "    \n",
        "    def fit(self, nepochs):\n",
        "        best_valid_loss = float('inf')\n",
        "\n",
        "        for epoch in tqdm(range(nepochs)):\n",
        "            start_time = time.time()\n",
        "            train_loss = self.train()\n",
        "            valid_loss = self.evaluate(self.valid_iterator)\n",
        "            end_time = time.time()\n",
        "            epoch_mins, epoch_secs = self.epoch_time(start_time, end_time)\n",
        "\n",
        "            if valid_loss < best_valid_loss:\n",
        "                best_valid_loss = valid_loss\n",
        "                # torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "                print(f'Epoch with best validation loss: {epoch+1:02}')\n",
        "\n",
        "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "            print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "            \n",
        "    def predict(self, iterator):\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(tqdm(iterator)):\n",
        "\n",
        "                src = batch.text\n",
        "                trg = batch.summary\n",
        "                output = self.model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "                #trg = [trg len, batch size]\n",
        "                #output = [trg len, batch size, output dim]\n",
        "                \n",
        "                if i == 0:\n",
        "                    outputs = torch.argmax(output, -1)\n",
        "                else:\n",
        "                    outputs = torch.cat((outputs, torch.argmax(output, -1)), -1)\n",
        "                \n",
        "                # outputs = [trg_len, len(iterator)]\n",
        "        return torch.transpose(outputs, 0, 1)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6mTEVla8u9g",
        "outputId": "7ba53c59-e8f2-4361-8637-eea22bb3af10"
      },
      "source": [
        "# config vaiables\n",
        "pad_index = SUMMARY.vocab.stoi[SUMMARY.pad_token]\n",
        "# initialize trainer\n",
        "trainer = Seq2Seq_trainer(model, train_iterator, valid_iterator, pad_index, device, 1, 1e-3)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 6,504,190 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712,
          "referenced_widgets": [
            "2148bb51fd1b407a9f358dbbffe019f4",
            "3a26e6d8c5ff4bb7b066d9e1b714c4f7",
            "1ab81ae778d346fbad973d2b0b014434",
            "b31f239b8ca84fa3ad4662454908e397",
            "3c89c411b3c04d1da181edde6e68c29b",
            "b4124fac8e234d3ebfbed0ffc9cad63c",
            "3c3ed35ec87b40d39ef20912e3c7e10a",
            "f9acee365c0940918425f506a21eb6c9"
          ]
        },
        "id": "Bel8WuKl9GXO",
        "outputId": "8438feca-3b13-4998-aea0-e4fa125f2a3f"
      },
      "source": [
        "trainer.fit(10)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2148bb51fd1b407a9f358dbbffe019f4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch with best validation loss: 01\n",
            "Epoch: 01 | Time: 2m 3s\n",
            "\tTrain Loss: 6.689 | Train PPL: 803.133\n",
            "\t Val. Loss: 6.395 |  Val. PPL: 598.689\n",
            "Epoch with best validation loss: 02\n",
            "Epoch: 02 | Time: 2m 2s\n",
            "\tTrain Loss: 6.289 | Train PPL: 538.663\n",
            "\t Val. Loss: 6.393 |  Val. PPL: 597.541\n",
            "Epoch: 03 | Time: 2m 2s\n",
            "\tTrain Loss: 6.223 | Train PPL: 504.256\n",
            "\t Val. Loss: 6.405 |  Val. PPL: 604.726\n",
            "Epoch: 04 | Time: 2m 2s\n",
            "\tTrain Loss: 6.164 | Train PPL: 475.372\n",
            "\t Val. Loss: 6.403 |  Val. PPL: 603.850\n",
            "Epoch with best validation loss: 05\n",
            "Epoch: 05 | Time: 2m 2s\n",
            "\tTrain Loss: 6.042 | Train PPL: 420.898\n",
            "\t Val. Loss: 6.247 |  Val. PPL: 516.268\n",
            "Epoch with best validation loss: 06\n",
            "Epoch: 06 | Time: 2m 2s\n",
            "\tTrain Loss: 5.822 | Train PPL: 337.764\n",
            "\t Val. Loss: 6.160 |  Val. PPL: 473.362\n",
            "Epoch with best validation loss: 07\n",
            "Epoch: 07 | Time: 2m 2s\n",
            "\tTrain Loss: 5.654 | Train PPL: 285.362\n",
            "\t Val. Loss: 6.060 |  Val. PPL: 428.256\n",
            "Epoch with best validation loss: 08\n",
            "Epoch: 08 | Time: 2m 2s\n",
            "\tTrain Loss: 5.467 | Train PPL: 236.654\n",
            "\t Val. Loss: 5.982 |  Val. PPL: 396.062\n",
            "Epoch with best validation loss: 09\n",
            "Epoch: 09 | Time: 2m 2s\n",
            "\tTrain Loss: 5.305 | Train PPL: 201.290\n",
            "\t Val. Loss: 5.938 |  Val. PPL: 379.069\n",
            "Epoch with best validation loss: 10\n",
            "Epoch: 10 | Time: 2m 2s\n",
            "\tTrain Loss: 5.171 | Train PPL: 176.026\n",
            "\t Val. Loss: 5.883 |  Val. PPL: 358.754\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr53AzWU9wmo",
        "outputId": "715683f8-9443-4437-a9a3-38b9675bf033"
      },
      "source": [
        "# evaluate on test data\n",
        "test_loss = trainer.evaluate(test_iterator)\n",
        "print(f'\\t Test. Loss: {test_loss:.3f} |  Test. PPL: {math.exp(test_loss):7.3f}')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t Test. Loss: 5.871 |  Test. PPL: 354.561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ebw98ig1GH4Q"
      },
      "source": [
        "body = '''\n",
        "       Scientists say they have discovered a new species of orangutans on Indonesia’s island of Sumatra.\n",
        "The population differs in several ways from the two existing orangutan species found in Sumatra and the neighboring island of Borneo.\n",
        "The orangutans were found inside North Sumatra’s Batang Toru forest, the science publication Current Biology reported.\n",
        "Researchers named the new species the Tapanuli orangutan. They say the animals are considered a new species because of genetic, skeletal and tooth differences.\n",
        "Michael Kruetzen is a geneticist with the University of Zurich who has studied the orangutans for several years. He said he was excited to be part of the unusual discovery of a new great ape in the present day. He noted that most great apes are currently considered endangered or severely endangered.\n",
        "Gorillas, chimpanzees and bonobos also belong to the great ape species.\n",
        "Orangutan – which means person of the forest in the Indonesian and Malay languages - is the world’s biggest tree-living mammal. The orange-haired animals can move easily among the trees because their arms are longer than their legs. They live more lonely lives than other great apes, spending a lot of time sleeping and eating fruit in the forest.\n",
        "The new study said fewer than 800 of the newly-described orangutans exist. Their low numbers make the group the most endangered of all the great ape species.\n",
        "They live within an area covering about 1,000 square kilometers. The population is considered highly vulnerable. That is because the environment which they depend on is greatly threatened by development.\n",
        "Researchers say if steps are not taken quickly to reduce the current and future threats, the new species could become extinct “within our lifetime.”\n",
        "Research into the new species began in 2013, when an orangutan protection group in Sumatra found an injured orangutan in an area far away from the other species. The adult male orangutan had been beaten by local villagers and died of his injuries. The complete skull was examined by researchers.\n",
        "Among the physical differences of the new species are a notably smaller head and frizzier hair. The Tapanuli orangutans also have a different diet and are found only in higher forest areas.\n",
        "There is no unified international system for recognizing new species. But to be considered, discovery claims at least require publication in a major scientific publication.\n",
        "Russell Mittermeier is head of the primate specialist group at the International Union for the Conservation of Nature. He called the finding a “remarkable discovery.” He said it puts responsibility on the Indonesian government to help the species survive.\n",
        "Matthew Nowak is one of the writers of the study. He told the Associated Press that there are three groups of the Tapanuli orangutans that are separated by non-protected land.He said forest land needs to connect the separated groups.\n",
        "In addition, the writers of the study are recommending that plans for a hydropower center in the area be stopped by the government.\n",
        "It also recommended that remaining forest in the Sumatran area where the orangutans live be protected.\n",
        "I’m Bryan Lynn.\n",
        "\n",
        "        '''\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315,
          "referenced_widgets": [
            "6afaa31c3a974275923c653865c528ce",
            "83b8dd1eb6fc4dfd84e9b0f80587897a",
            "a63ab65c4d344e038c456a3a35a12c23",
            "0bd8eb7956bd4233939788d0c0a2cb39",
            "21ff8993bad34161a8a3f34a6bbe7f1d",
            "15b526fe3097482dbd3ae76f01610d31",
            "9d214df0bd57460a836254092067a87f",
            "f7745ed739cc46129fde039120a58a01"
          ]
        },
        "id": "UWREfU4QGVF3",
        "outputId": "6df84f34-9307-41fb-cec0-790f5a2b3911"
      },
      "source": [
        "test_tensor = trainer.predict(test_iterator)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6afaa31c3a974275923c653865c528ce",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=97.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-f589757d1947>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-2e45d535996c>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;31m# outputs = [trg_len, len(iterator)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Got 13 and 14 (The offending index is 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmBt-PVVH8Xt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}