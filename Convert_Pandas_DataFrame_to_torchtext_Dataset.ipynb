{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convert_Pandas_DataFrame_to_torchtext_Dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNEGqgepycCT6SkRc4o1gk8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoyElkabetz/Text-Summarization-with-Deep-Learning/blob/main/Convert_Pandas_DataFrame_to_torchtext_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYMoY3nHG63J",
        "outputId": "aa59627a-d4e1-4b4e-9a03-82e510121df1"
      },
      "source": [
        "## uncomment if you want to mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "jNUoUKxVHDeY",
        "outputId": "623d79de-8ac9-4c46-fc5e-8bb840d46b87"
      },
      "source": [
        "PATH_DATASET = '/content/gdrive/MyDrive/Datasets/Text/news_summary_more.csv'\n",
        "my_data = pd.read_csv(PATH_DATASET ,encoding='utf-8')\n",
        "my_data.drop_duplicates(subset=['text'],inplace=True)#dropping duplicates\n",
        "my_data.dropna(axis=0,inplace=True)#dropping na\n",
        "my_data.head()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
              "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
              "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
              "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
              "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
              "      <td>Speaking about the sexual harassment allegatio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           headlines                                               text\n",
              "0  upGrad learner switches to career in ML & Al w...  Saurav Kant, an alumnus of upGrad and IIIT-B's...\n",
              "1  Delhi techie wins free food from Swiggy for on...  Kunal Shah's credit card bill payment platform...\n",
              "2  New Zealand end Rohit Sharma-led India's 12-ma...  New Zealand defeated India by 8 wickets in the...\n",
              "3  Aegon life iTerm insurance plan helps customer...  With Aegon Life iTerm Insurance plan, customer...\n",
              "4  Have known Hirani for yrs, what if MeToo claim...  Speaking about the sexual harassment allegatio..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc60hHsXEa_O"
      },
      "source": [
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import torchtext.legacy.data as data\n",
        "from torchtext.legacy.data import Field, Dataset, Example\n",
        "import pandas as pd\n",
        "\n",
        "class DataFrameDataset(Dataset):\n",
        "    \"\"\"Class for using pandas DataFrames as a datasource\"\"\"\n",
        "    \"\"\"Pytorch legacy Dataset: https://pytorch.org/text/_modules/torchtext/data/dataset.html\"\"\"\n",
        "    def __init__(self, examples, fields, filter_pred=None):\n",
        "        \"\"\"\n",
        "        Create a dataset from a pandas dataframe of examples and Fields\n",
        "        Arguments:\n",
        "            examples pd.DataFrame: DataFrame of examples\n",
        "            fields {str: Field}: The Fields to use in this tuple. The\n",
        "                string is a field name, and the Field is the associated field.\n",
        "            filter_pred (callable or None): use only exanples for which\n",
        "                filter_pred(example) is true, or use all examples if None.\n",
        "                Default is None\n",
        "        \"\"\"\n",
        "        self.examples = examples.apply(SeriesExample.fromSeries, args=(fields,), axis=1).tolist()\n",
        "        if filter_pred is not None:\n",
        "            self.examples = filter(filter_pred, self.examples)\n",
        "        self.fields = dict(fields)\n",
        "        # Unpack field tuples\n",
        "        for n, f in list(self.fields.items()):\n",
        "            if isinstance(n, tuple):\n",
        "                self.fields.update(zip(n, f))\n",
        "                del self.fields[n]\n",
        "\n",
        "class SeriesExample(Example):\n",
        "    \"\"\"Class to convert a pandas Series to an Example\"\"\"\n",
        "  \n",
        "    @classmethod\n",
        "    def fromSeries(cls, data, fields):\n",
        "        return cls.fromdict(data.to_dict(), fields)\n",
        "\n",
        "    @classmethod\n",
        "    def fromdict(cls, data, fields):\n",
        "        ex = cls()\n",
        "        \n",
        "        for key, field in fields.items():\n",
        "            if key not in data:\n",
        "                raise ValueError(\"Specified key {} was not found in \"\n",
        "                \"the input data\".format(key))\n",
        "            if field is not None:\n",
        "                setattr(ex, key, field.preprocess(data[key]))\n",
        "            else:\n",
        "                setattr(ex, key, data[key])\n",
        "        return ex"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxuZIu1RExN8"
      },
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "# TEXT = data.Field(sequential=True, tokenize='spacy')\n",
        "# LABEL = data.Field(sequential=False, dtype=torch.float)\n",
        "# get tokenizer\n",
        "\n",
        "# define the text field for the dataset\n",
        "TEXT = data.Field(sequential=True,\n",
        "                  lower=True, \n",
        "                  tokenize=tokenizer,\n",
        "                  init_token='<sos>', \n",
        "                  eos_token='<eos>',\n",
        "                  dtype=torch.long)\n",
        "# define the text field for the dataset\n",
        "SUMMARY = data.Field(sequential=True,\n",
        "                    lower=True, \n",
        "                    tokenize=tokenizer,\n",
        "                    init_token='<sos>', \n",
        "                    eos_token='<eos>',\n",
        "                    dtype=torch.long)\n",
        "#SUMMARY = data.Field(sequential=True, tokenize=tokenizer)\n",
        "# TEXT.build_vocab(my_data, max_size=25000, vectors=\"glove.6B.100d\") \n",
        "#TEXT.build_vocab(my_data, max_size=25000) \n",
        "#SUMMARY.build_vocab(my_data)\n",
        "fields = {'headlines': SUMMARY, 'text': TEXT}"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPXujl16GMg7"
      },
      "source": [
        "train_ds = DataFrameDataset(my_data, fields)\n",
        "# valid_ds = DataFrameDataset(valid_df, fields)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3pIxm9iNzuc",
        "outputId": "697a99a5-8b51-4bd8-aa53-0a43e2fbd26a"
      },
      "source": [
        "len(train_ds)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98360"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw9yp7beGMl7",
        "outputId": "3d84fe97-224e-4c61-82e4-c79518f099ca"
      },
      "source": [
        "print(\"Text: \", \" \".join([t for t in train_ds.examples[0].text]))\n",
        "print(\"Headline: \", \" \".join([t for t in train_ds.examples[0].headlines]))\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text:  saurav kant , an alumnus of upgrad and iiit-b ' s pg program in machine learning and artificial intelligence , was a sr systems engineer at infosys with almost 5 years of work experience . the program and upgrad ' s 360-degree career support helped him transition to a data scientist at tech mahindra with 90% salary hike . upgrad ' s online power learning has powered 3 lakh+ careers .\n",
            "Headline:  upgrad learner switches to career in ml & al with 90% salary hike\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpSS4y_4NP5y"
      },
      "source": [
        "train_dataset, test_dataset = DataFrameDataset(my_data, fields={'text': TEXT, 'headlines': SUMMARY}).split()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBooVhtmFYjY",
        "outputId": "af2d6733-df18-42d0-e6c8-07e94329226f"
      },
      "source": [
        "len(test_dataset)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29508"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxcvLWZNRPHS"
      },
      "source": [
        "def batchify(data, bsz, text_field):\n",
        "    data = text_field.numericalize([data.examples[0].text])\n",
        "    # Divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "uSF8X8l3RTb9",
        "outputId": "f8246c95-dfb1-4d63-a060-8896a85c6748"
      },
      "source": [
        "# get tokenizer\n",
        "tokenizer = data.utils.get_tokenizer(\"basic_english\")\n",
        "\n",
        "\n",
        "# download the WikiText2 dataset\n",
        "train_dataset, test_dataset = DataFrameDataset(my_data, fields={'text': TEXT, 'headlines': SUMMARY}).split()\n",
        "\n",
        "# create vocabulary\n",
        "TEXT.build_vocab(train_dataset)\n",
        "vocab = TEXT.vocab\n",
        "\n",
        "# split the data into batches\n",
        "batch_size = 20\n",
        "train_loader = batchify(train_dataset, batch_size, TEXT)\n",
        "#val_loader = batchify(val_Wiki2, batch_size, text)\n",
        "test_loader = batchify(test_dataset, batch_size, TEXT)\n",
        "display(HTML('<h4>Data loaders shapes:</h4>'))\n",
        "print(f'The train dataset shape is: {train_loader.shape}')\n",
        "#print(f'The validation dataset shape is: {val_loader.shape}')\n",
        "print(f'The test dataset shape is: {test_loader.shape}')\n",
        "\n",
        "# display data samples\n",
        "display(HTML('<h4>Display data samples:</h4>'))\n",
        "n_samples = 2\n",
        "for i in range(n_samples):\n",
        "    tokens = train_loader[i]\n",
        "    print(f'Sample {i}:')\n",
        "    print(f'Tokens: {list(tokens.cpu().numpy())}')\n",
        "    print(\"Text: \", \" \".join([vocab.itos[t] for t in tokens]))\n",
        "    print('\\n')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-6e8866eb8ef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# split the data into batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#val_loader = batchify(val_Wiki2, batch_size, text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vg5hj6gFYl5"
      },
      "source": [
        "# from torchtext import data\n",
        "\n",
        "# class DataFrameDataset(data.Dataset):\n",
        "\n",
        "#     def __init__(self, df, text_field, label_field, is_test=False, **kwargs):\n",
        "#         fields = [('text', text_field), ('label', label_field)]\n",
        "#         examples = []\n",
        "#         for i, row in df.iterrows():\n",
        "#             label = row.sentiment if not is_test else None\n",
        "#             text = row.text\n",
        "#             examples.append(data.Example.fromlist([text, label], fields))\n",
        "\n",
        "#         super().__init__(examples, fields, **kwargs)\n",
        "\n",
        "#     @staticmethod\n",
        "#     def sort_key(ex):\n",
        "#         return len(ex.text)\n",
        "\n",
        "#     @classmethod\n",
        "#     def splits(cls, text_field, label_field, train_df, val_df=None, test_df=None, **kwargs):\n",
        "#         train_data, val_data, test_data = (None, None, None)\n",
        "\n",
        "#         if train_df is not None:\n",
        "#             train_data = cls(train_df.copy(), text_field, label_field, **kwargs)\n",
        "#         if val_df is not None:\n",
        "#             val_data = cls(val_df.copy(), text_field, label_field, **kwargs)\n",
        "#         if test_df is not None:\n",
        "#             test_data = cls(test_df.copy(), text_field, label_field, True, **kwargs)\n",
        "\n",
        "#         return tuple(d for d in (train_data, val_data, test_data) if d is not None)\n",
        "        \n",
        "# train_ds, val_ds, test_ds = DataFrameDataset.splits(\n",
        "# text_field=TEXT_FIELD, label_field=LABEL_FIELD, train_df=train_df, val_df=val_df, test_df=test_df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}